<!DOCTYPE html>
<html lang="en"><head>
<script src="index_files/libs/clipboard/clipboard.min.js"></script>
<script src="index_files/libs/quarto-html/tabby.min.js"></script>
<script src="index_files/libs/quarto-html/popper.min.js"></script>
<script src="index_files/libs/quarto-html/tippy.umd.min.js"></script>
<link href="index_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="index_files/libs/quarto-html/light-border.css" rel="stylesheet">
<link href="index_files/libs/quarto-html/quarto-html.min.css" rel="stylesheet" data-mode="light">
<link href="index_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.3.450">

  <title>index</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="index_files/libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="index_files/libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
    /* CSS for syntax highlighting */
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #767676;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #767676;  padding-left: 4px; }
    div.sourceCode
      { color: #545454; background-color: #fefefe; }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span { color: #545454; } /* Normal */
    code span.al { color: #7928a1; } /* Alert */
    code span.an { color: #696969; } /* Annotation */
    code span.at { color: #a55a00; } /* Attribute */
    code span.bn { color: #7928a1; } /* BaseN */
    code span.bu { } /* BuiltIn */
    code span.cf { color: #d91e18; } /* ControlFlow */
    code span.ch { color: #008000; } /* Char */
    code span.cn { color: #d91e18; } /* Constant */
    code span.co { color: #696969; } /* Comment */
    code span.cv { color: #696969; font-style: italic; } /* CommentVar */
    code span.do { color: #696969; font-style: italic; } /* Documentation */
    code span.dt { color: #7928a1; } /* DataType */
    code span.dv { color: #7928a1; } /* DecVal */
    code span.er { color: #7928a1; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #a55a00; } /* Float */
    code span.fu { color: #06287e; } /* Function */
    code span.im { } /* Import */
    code span.in { color: #696969; } /* Information */
    code span.kw { color: #d91e18; } /* Keyword */
    code span.op { color: #00769e; } /* Operator */
    code span.ot { color: #d91e18; } /* Other */
    code span.pp { color: #7928a1; } /* Preprocessor */
    code span.sc { color: #00769e; } /* SpecialChar */
    code span.ss { color: #008000; } /* SpecialString */
    code span.st { color: #008000; } /* String */
    code span.va { color: #a55a00; } /* Variable */
    code span.vs { color: #008000; } /* VerbatimString */
    code span.wa { color: #696969; font-style: italic; } /* Warning */
    /* CSS for citations */
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
    }
    .hanging-indent div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }  </style>
  <link rel="stylesheet" href="index_files/libs/revealjs/dist/theme/quarto.css">
  <link href="index_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="index_files/libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="index_files/libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="index_files/libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">

  .callout {
    margin-top: 1em;
    margin-bottom: 1em;  
    border-radius: .25rem;
  }

  .callout.callout-style-simple { 
    padding: 0em 0.5em;
    border-left: solid #acacac .3rem;
    border-right: solid 1px silver;
    border-top: solid 1px silver;
    border-bottom: solid 1px silver;
    display: flex;
  }

  .callout.callout-style-default {
    border-left: solid #acacac .3rem;
    border-right: solid 1px silver;
    border-top: solid 1px silver;
    border-bottom: solid 1px silver;
  }

  .callout .callout-body-container {
    flex-grow: 1;
  }

  .callout.callout-style-simple .callout-body {
    font-size: 1rem;
    font-weight: 400;
  }

  .callout.callout-style-default .callout-body {
    font-size: 0.9rem;
    font-weight: 400;
  }

  .callout.callout-titled.callout-style-simple .callout-body {
    margin-top: 0.2em;
  }

  .callout:not(.callout-titled) .callout-body {
      display: flex;
  }

  .callout:not(.no-icon).callout-titled.callout-style-simple .callout-content {
    padding-left: 1.6em;
  }

  .callout.callout-titled .callout-header {
    padding-top: 0.2em;
    margin-bottom: -0.2em;
  }

  .callout.callout-titled .callout-title  p {
    margin-top: 0.5em;
    margin-bottom: 0.5em;
  }
    
  .callout.callout-titled.callout-style-simple .callout-content  p {
    margin-top: 0;
  }

  .callout.callout-titled.callout-style-default .callout-content  p {
    margin-top: 0.7em;
  }

  .callout.callout-style-simple div.callout-title {
    border-bottom: none;
    font-size: .9rem;
    font-weight: 600;
    opacity: 75%;
  }

  .callout.callout-style-default  div.callout-title {
    border-bottom: none;
    font-weight: 600;
    opacity: 85%;
    font-size: 0.9rem;
    padding-left: 0.5em;
    padding-right: 0.5em;
  }

  .callout.callout-style-default div.callout-content {
    padding-left: 0.5em;
    padding-right: 0.5em;
  }

  .callout.callout-style-simple .callout-icon::before {
    height: 1rem;
    width: 1rem;
    display: inline-block;
    content: "";
    background-repeat: no-repeat;
    background-size: 1rem 1rem;
  }

  .callout.callout-style-default .callout-icon::before {
    height: 0.9rem;
    width: 0.9rem;
    display: inline-block;
    content: "";
    background-repeat: no-repeat;
    background-size: 0.9rem 0.9rem;
  }

  .callout-title {
    display: flex
  }
    
  .callout-icon::before {
    margin-top: 1rem;
    padding-right: .5rem;
  }

  .callout.no-icon::before {
    display: none !important;
  }

  .callout.callout-titled .callout-body > .callout-content > :last-child {
    margin-bottom: 0.5rem;
  }

  .callout.callout-titled .callout-icon::before {
    margin-top: .5rem;
    padding-right: .5rem;
  }

  .callout:not(.callout-titled) .callout-icon::before {
    margin-top: 1rem;
    padding-right: .5rem;
  }

  /* Callout Types */

  div.callout-note {
    border-left-color: #4582ec !important;
  }

  div.callout-note .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAEU0lEQVRYCcVXTWhcVRQ+586kSUMMxkyaElstCto2SIhitS5Ek8xUKV2poatCcVHtUlFQk8mbaaziwpWgglJwVaquitBOfhQXFlqlzSJpFSpIYyXNjBNiTCck7x2/8/LeNDOZxDuEkgOXe++553zfefee+/OYLOXFk3+1LLrRdiO81yNqZ6K9cG0P3MeFaMIQjXssE8Z1JzLO9ls20MBZX7oG8w9GxB0goaPrW5aNMp1yOZIa7Wv6o2ykpLtmAPs/vrG14Z+6d4jpbSKuhdcSyq9wGMPXjonwmESXrriLzFGOdDBLB8Y6MNYBu0dRokSygMA/mrun8MGFN3behm6VVAwg4WR3i6FvYK1T7MHo9BK7ydH+1uurECoouk5MPRyVSBrBHMYwVobG2aOXM07sWrn5qgB60rc6mcwIDJtQrnrEr44kmy+UO9r0u9O5/YbkS9juQckLed3DyW2XV/qWBBB3ptvI8EUY3I9p/67OW+g967TNr3Sotn3IuVlfMLVnsBwH4fsnebJvyGm5GeIUA3jljERmrv49SizPYuq+z7c2H/jlGC+Ghhupn/hcapqmcudB9jwJ/3jvnvu6vu5lVzF1fXyZuZZ7U8nRmVzytvT+H3kilYvH09mLWrQdwFSsFEsxFVs5fK7A0g8gMZjbif4ACpKbjv7gNGaD8bUrlk8x+KRflttr22JEMRUbTUwwDQScyzPgedQHZT0xnx7ujw2jfVfExwYHwOsDTjLdJ2ebmeQIlJ7neo41s/DrsL3kl+W2lWvAga0tR3zueGr6GL78M3ifH0rGXrBC2aAR8uYcIA5gwV8zIE8onoh8u0Fca/ciF7j1uOzEnqcIm59sEXoGc0+z6+H45V1CvAvHcD7THztu669cnp+L0okAeIc6zjbM/24LgGM1gZk7jnRu1aQWoU9sfUOuhrmtaPIO3YY1KLLWZaEO5TKUbMY5zx8W9UJ6elpLwKXbsaZ4EFl7B4bMtDv0iRipKoDQT2sNQI9b1utXFdYisi+wzZ/ri/1m7QfDgEuvgUUEIJPq3DhX/5DWNqIXDOweC2wvIR90Oq3lDpdMIgD2r0dXvGdsEW5H6x6HLRJYU7C69VefO1x8Gde1ZFSJLfWS1jbCnhtOPxmpfv2LXOA2Xk2tvnwKKPFuZ/oRmwBwqRQDcKNeVQkYcOjtWVBuM/JuYw5b6isojIkYxyYAFn5K7ZBF10fea52y8QltAg6jnMqNHFBmGkQ1j+U43HMi2xMar1Nv0zGsf1s8nUsmUtPOOrbFIR8bHFDMB5zL13Gmr/kGlCkUzedTzzmzsaJXhYawnA3UmARpiYj5ooJZiUoxFRtK3X6pgNPv+IZVPcnwbOl6f+aBaO1CNvPW9n9LmCp01nuSaTRF2YxHqZ8DYQT6WsXT+RD6eUztwYLZ8rM+rcPxamv1VQzFUkzFXvkiVrySGQgJNvXHJAxiU3/NwiC03rSf05VBaPtu/Z7/B8Yn/w7eguloAAAAAElFTkSuQmCC');
  }

  div.callout-note.callout-style-default .callout-title {
    background-color: #dae6fb
  }

  div.callout-important {
    border-left-color: #d9534f !important;
  }

  div.callout-important .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAEKklEQVRYCcVXTWhcVRS+575MJym48A+hSRFr00ySRQhURRfd2HYjk2SSTokuBCkU2o0LoSKKraKIBTcuFCoidGFD08nkBzdREbpQ1EDNIv8qSGMFUboImMSZd4/f9zJv8ibJMC8xJQfO3HPPPef7zrvvvnvviIkpC9nsw0UttFunbUhpFzFtarSd6WJkStVMw5xyVqYTvkwfzuf/5FgtkVoB0729j1rjXwThS7Vio+Mo6DNnvLfahoZ+i/o32lULuJ3NNiz7q6+pyAUkJaFF6JwaM2lUJlV0MlnQn5aTRbEu0SEqHUa0A4AdiGuB1kFXRfVyg5d87+Dg4DL6m2TLAub60ilj7A1Ec4odSAc8X95sHh7+ZRPCFo6Fnp7HfU/fBng/hi10CjCnWnJjsxvDNxWw0NfV6Rv5GgP3I3jGWXumdTD/3cbEOP2ZbOZp69yniG3FQ9z1jD7bnBu9Fc2tKGC2q+uAJOQHBDRiZX1x36o7fWBs7J9ownbtO+n0/qWkvW7UPIfc37WgT6ZGR++EOJyeQDSb9UB+DZ1G6DdLDzyS+b/kBCYGsYgJbSQHuThGKRcw5xdeQf8YdNHsc6ePXrlSYMBuSIAFTGAtQo+VuALo4BX83N190NWZWbynBjhOHsmNfFWLeL6v+ynsA58zDvvAC8j5PkbOcXCMg2PZFk3q8MjI7WAG/Dp9AwP7jdGBOOQkAvlFUB+irtm16I1Zw9YBcpGTGXYmk3kQIC/Cds55l+iMI3jqhjAuaoe+am2Jw5GT3Nbz3CkE12NavmzN5+erJW7046n/CH1RO/RVa8lBLozXk9uqykkGAyRXLWlLv5jyp4RFsG5vGVzpDLnIjTWgnRy2Rr+tDKvRc7Y8AyZq10jj8DqXdnIRNtFZb+t/ZRtXcDiVnzpqx8mPcDWxgARUqx0W1QB9MeUZiNrV4qP+Ehc+BpNgATsTX8ozYKL2NtFYAHc84fG7ndxUPr+AR/iQSns7uSUufAymwDOb2+NjK27lEFocm/EE2WpyIy/Hi66MWuMKJn8RvxIcj87IM5Vh9663ziW36kR0HNenXuxmfaD8JC7tfKbrhFr7LiZCrMjrzTeGx+PmkosrkNzW94ObzwocJ7A1HokLolY+AvkTiD/q1H0cN48c5EL8Crkttsa/AXQVDmutfyku0E7jShx49XqV3MFK8IryDhYVbj7Sj2P2eBxwcXoe8T8idsKKPRcnZw1b+slFTubwUwhktrfnAt7J++jwQtLZcm3sr9LQrjRzz6cfMv9aLvgmnAGvpoaGLxM4mAEaLV7iAzQ3oU0IvD5x9ix3yF2RAAuYAOO2f7PEFWCXZ4C9Pb2UsgDeVnFSpbFK7/IWu7TPTvBqzbGdCHOJQSxiEjt6IyZmxQyEJHv6xyQsYk//moVFsN2zP6fRImjfq7/n/wFDguUQFNEwugAAAABJRU5ErkJggg==');
  }

  div.callout-important.callout-style-default .callout-title {
    background-color: #f7dddc
  }

  div.callout-warning {
    border-left-color: #f0ad4e !important;
  }

  div.callout-warning .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAETklEQVRYCeVWW2gcVRg+58yaTUnizqbipZeX4uWhBEniBaoUX1Ioze52t7sRq6APio9V9MEaoWlVsFasRq0gltaAPuxms8lu0gcviE/FFOstVbSIxgcv6SU7EZqmdc7v9+9mJtNks51NTUH84ed889/PP+cmxP+d5FIbMJmNbpREu4WUkiTtCicKny0l1pIKmBzovF2S+hIJHX8iEu3hZJ5lNZGqyRrGSIQpq15AzF28jgpeY6yk6GVdrfFqdrD6Iw+QlB8g0YS2g7dyQmXM/IDhBhT0UCiRf59lfqmmDvzRt6kByV/m4JjtzuaujMUM2c5Z2d6JdKrRb3K2q6mA+oYVz8JnDdKPmmNthzkAk/lN63sYPgevrguc72aZX/L9C6x09GYyxBgCX4NlvyGUHOKELlm5rXeR1kchuChJt4SSwyddZRXgvwMGvYo4QSlk3/zkHD8UHxwVJA6zjZZqP8v8kK8OWLnIZtLyCAJagYC4rTGW/9Pqj92N/c+LUaAj27movwbi19tk/whRCIE7Q9vyI6yvRpftAKVTdUjOW40X3h5OXsKCdmFcx0xlLJoSuQngnrJe7Kcjm4OMq9FlC7CMmScQANuNvjfP3PjGXDBaUQmbp296S5L4DrpbrHN1T87ZVEZVCzg1FF0Ft+dKrlLukI+/c9ENo+TvlTDbYFvuKPtQ9+l052rXrgKoWkDAFnvh0wTOmYn8R5f4k/jN/fZiCM1tQx9jQQ4ANhqG4hiL0qIFTGViG9DKB7GYzgubnpofgYRwO+DFjh0Zin2m4b/97EDkXkc+f6xYAPX0KK2I/7fUQuwzuwo/L3AkcjugPNixC8cHf0FyPjWlItmLxWw4Ou9YsQCr5fijMGoD/zpdRy95HRysyXA74MWOnscpO4j2y3HAVisw85hX5+AFBRSHt4ShfLFkIMXTqyKFc46xdzQM6XbAi702a7sy04J0+feReMFKp5q9esYLCqAZYw/k14E/xcLLsFElaornTuJB0svMuJINy8xkIYuL+xPAlWRceH6+HX7THJ0djLUom46zREu7tTkxwmf/FdOZ/sh6Q8qvEAiHpm4PJ4a/doJe0gH1t+aHRgCzOvBvJedEK5OFE5jpm4AGP2a8Dxe3gGJ/pAutug9Gp6he92CsSsWBaEcxGx0FHytmIpuqGkOpldqNYQK8cSoXvd+xLxXADw0kf6UkJNFtdo5MOgaLjiQOQHcn+A6h5NuL2s0qsC2LOM75PcF3yr5STuBSAcGG+meA14K/CI21HcS4LBT6tv0QAh8Dr5l93AhZzG5ZJ4VxAqdZUEl9z7WJ4aN+svMvwHHL21UKTd1mqvChH7/Za5xzXBBKrUcB0TQ+Ulgkfbi/H/YT5EptrGzsEK7tR1B7ln9BBwckYfMiuSqklSznIuoIIOM42MQO+QnduCoFCI0bpkzjCjddHPN/F+2Yu+sd9bKNpVwHhbS3LluK/0zgfwD0xYI5dXuzlQAAAABJRU5ErkJggg==');
  }

  div.callout-warning.callout-style-default .callout-title {
    background-color: #fcefdc
  }

  div.callout-tip {
    border-left-color: #02b875 !important;
  }

  div.callout-tip .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAADr0lEQVRYCe1XTWgTQRj9ZjZV8a9SPIkKgj8I1bMHsUWrqYLVg4Ue6v9BwZOxSYsIerFao7UiUryIqJcqgtpimhbBXoSCVxUFe9CTiogUrUp2Pt+3aUI2u5vdNh4dmMzOzHvvezuz8xNFM0mjnbXaNu1MvFWRXkXEyE6aYOYJpdW4IXuA4r0fo8qqSMDBU0v1HJUgVieAXxzCsdE/YJTdFcVIZQNMyhruOMJKXYFoLfIfIvVIMWdsrd+Rpd86ZmyzzjJmLStqRn0v8lzkb4rVIXvnpScOJuAn2ACC65FkPzEdEy4TPWRLJ2h7z4cArXzzaOdKlbOvKKX25Wl00jSnrwVxAg3o4dRxhO13RBSdNvH0xSARv3adTXbBdTf64IWO2vH0LT+cv4GR1DJt+DUItaQogeBX/chhbTBxEiZ6gftlDNXTrvT7co4ub5A6gp9HIcHvzTa46OS5fBeP87Qm0fQkr4FsYgVQ7Qg+ZayaDg9jhg1GkWj8RG6lkeSacrrHgDaxdoBiZPg+NXV/KifMuB6//JmYH4CntVEHy/keA6x4h4CU5oFy8GzrBS18cLJMXcljAKB6INjWsRcuZBWVaS3GDrqB7rdapVIeA+isQ57Eev9eCqzqOa81CY05VLd6SamW2wA2H3SiTbnbSxmzfp7WtKZkqy4mdyAlGx7ennghYf8voqp9cLSgKdqNfa6RdRsAAkPwRuJZNbpByn+RrJi1RXTwdi8RQF6ymDwGMAtZ6TVE+4uoKh+MYkcLsT0Hk8eAienbiGdjJHZTpmNjlbFJNKDVAp2fJlYju6IreQxQ08UJDNYdoLSl6AadO+fFuCQqVMB1NJwPm69T04Wv5WhfcWyfXQB+wXRs1pt+nCknRa0LVzSA/2B+a9+zQJadb7IyyV24YAxKp2Jqs3emZTuNnKxsah+uabKbMk7CbTgJx/zIgQYErIeTKRQ9yD9wxVof5YolPHqaWo7TD6tJlh7jQnK5z2n3+fGdggIOx2kaa2YI9QWarc5Ce1ipNWMKeSG4DysFF52KBmTNMmn5HqCFkwy34rDg05gDwgH3bBi+sgFhN/e8QvRn8kbamCOhgrZ9GJhFDgfcMHzFb6BAtjKpFhzTjwv1KCVuxHvCbsSiEz4CANnj84cwHdFXAbAOJ4LTSAawGWFn5tDhLMYz6nWeU2wJfIhmIJBefcd/A5FWQWGgrWzyORZ3Q6HuV+Jf0Bj+BTX69fm1zWgK7By1YTXchFDORywnfQ7GpzOo6S+qECrsx2ifVQAAAABJRU5ErkJggg==');
  }

  div.callout-tip.callout-style-default .callout-title {
    background-color: #ccf1e3
  }

  div.callout-caution {
    border-left-color: #fd7e14 !important;
  }

  div.callout-caution .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAACV0lEQVRYCdVWzWoUQRCuqp2ICBLJXgITZL1EfQDBW/bkzUMUD7klD+ATSHBEfAIfQO+iXsWDxJsHL96EHAwhgzlkg8nBg25XWb0zIb0zs9muYYWkoKeru+vn664fBqElyZNuyh167NXJ8Ut8McjbmEraKHkd7uAnAFku+VWdb3reSmRV8PKSLfZ0Gjn3a6Xlcq9YGb6tADjn+lUfTXtVmaZ1KwBIvFI11rRXlWlatwIAAv2asaa9mlB9wwygiDX26qaw1yYPzFXg2N1GgG0FMF8Oj+VIx7E/03lHx8UhvYyNZLN7BwSPgekXXLribw7w5/c8EF+DBK5idvDVYtEEwMeYefjjLAdEyQ3M9nfOkgnPTEkYU+sxMq0BxNR6jExrAI31H1rzvLEfRIdgcv1XEdj6QTQAS2wtstEALLG1yEZ3QhH6oDX7ExBSFEkFINXH98NTrme5IOaaA7kIfiu2L8A3qhH9zRbukdCqdsA98TdElyeMe5BI8Rs2xHRIsoTSSVFfCFCWGPn9XHb4cdobRIWABNf0add9jakDjQJpJ1bTXOJXnnRXHRf+dNL1ZV1MBRCXhMbaHqGI1JkKIL7+i8uffuP6wVQAzO7+qVEbF6NbS0LJureYcWXUUhH66nLR5rYmva+2tjRFtojkM2aD76HEGAD3tPtKM309FJg5j/K682ywcWJ3PASCcycH/22u+Bh7Aa0ehM2Fu4z0SAE81HF9RkB21c5bEn4Dzw+/qNOyXr3DCTQDMBOdhi4nAgiFDGCinIa2owCEChUwD8qzd03PG+qdW/4fDzjUMcE1ZpIAAAAASUVORK5CYII=');
  }

  div.callout-caution.callout-style-default .callout-title {
    background-color: #ffe5d0
  }

  </style>
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
  <meta name="twitter:title" content="Outrageously efficient exploratory data analysis with Apache Arrow and dplyr">
  <meta name="twitter:description" content="A 10 minute lightning talk on all things arrow + dplyr">
  <meta name="twitter:url" content="https://jthomasmock.github.io/arrow-dplyr/#/">
  <meta name="twitter:image" content="https://raw.githubusercontent.com/jthomasmock/arrow-dplyr/master/index-img.png">
  <meta name="twitter:image:alt" content="The title slide of the presentation, with the arrow and dplyr hex logos">
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:creator" content="@thomas_mock">
  <meta name="twitter:site" content="@thomas_mock">
  <meta property="og:title" content="Outrageously efficient exploratory data analysis with Apache Arrow and dplyr">
  <meta property="og:description" content="A 10 minute lightning talk on all things arrow + dplyr">
  <meta property="og:url" content="https://jthomasmock.github.io/arrow-dplyr/#/">
  <meta property="og:image" content="https://raw.githubusercontent.com/jthomasmock/arrow-dplyr/master/index-img.png">
  <meta property="og:image:alt" content="The title slide of the presentation, with the arrow and dplyr hex logos">
  <meta property="og:type" content="website">
  <meta property="og:locale" content="en_US">
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">


<section class="slide level2">

<h1>
Chaining Language and Knowledge Resources with LLM(s)
</h1>
<h2>
a tutorial at ROCLING 2023
</h2>
<hr>
<br>
<h3>
Shu-Kai Hsieh 謝舒凱
</h3>
<h4>
Graduate Institute of Linguistics/Brain and Mind, NTU
</h4>
<br>
<h3>
Piner Chen and Dachen Lian 陳品而 連大成
</h3>
<h4>
Graduate Institute of Linguistics, NTU
</h4>
<p><br></p>
<p><img data-src="https://lopentu.github.io/CwnWeb/assets/img/cwn-logo-main.svg" class="absolute" style="top: 600px; left: 1250px; width: 400px; "></p>
<p><img data-src="langchain.webp" class="absolute" style="top: 800px; left: 950px; width: 550px; "></p>
<!-- # Where all starts with .. -->
<!-- . . .  -->
<!-- ![](huang_cite.png){width="40%"} -->
</section>
<section id="今天要談的主題" class="title-slide slide level1">
<h1>今天要談的主題</h1>
<ul>
<li><p>Language Resources and Large Language Models: possible linkages</p></li>
<li><p>Hands-on code session</p></li>
</ul>

<img data-src="talk_qr.png" width="70" class="r-stretch"></section>

<section>
<section id="背景" class="title-slide slide level1">
<h1>背景</h1>
<p><span id="magenta">Language Models</span></p>
<div class="fragment">
<ul>
<li><p>Probability distributions over sequences of words, tokens or characters (Shannon, 1948; 1951)</p></li>
<li><p>As a core task in NLP, often framed as <strong>next token prediction</strong>.</p></li>
</ul>
</div>
</section>
<section id="預訓練大型語言模型橫空出世" class="slide level2">
<h2>預訓練大型語言模型橫空出世</h2>
<p><span id="magenta">Pre-trained Large Language Models</span></p>
<div class="fragment">
<ul>
<li>Transformer-based pre-trained Large Language Models changed NLP/the world</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="llm_tree.png" style="width:50.0%"></p>
</figure>
</div>
</div>
</section>
<section id="section" class="slide level2">
<h2></h2>
<ul>
<li><strong>Emergent Abilities</strong> in LLM : Unpredictable Abilities in Large Language Models From generation to understanding?</li>
</ul>
<p>直到大型語言模型開始出現結構理解的<em>頓悟</em>行為 (emergence)之後， 開始有推理能力的期待。</p>

<img data-src="emergence.png" style="width:2.0%" class="r-stretch quarto-figure-center"></section>
<section id="science-exam-reasoning" class="slide level2">
<h2>Science exam reasoning</h2>
<div class="fragment">
<p><img data-src="reasoning.png"></p>
</div>
</section>
<section id="mathematical-reasoning" class="slide level2">
<h2><a href="https://openai.com/research/formal-math">Mathematical reasoning</a></h2>
<div class="fragment">
<p><img data-src="reasoning2.png" style="width:70.0%"></p>
</div>
</section>
<section id="iterative-reasoning-and-cultural-imagination" class="slide level2">
<h2>Iterative Reasoning and Cultural Imagination</h2>
<p><span id="magenta">Rosetta Stone Problems</span></p>
<div class="fragment">
<p><img data-src="iol2.png"></p>
</div>
</section>
<section id="跨符碼類型推理" class="slide level2">
<h2>跨符碼類型推理</h2>
<p><img data-src="iol1.png"> <img data-src="metrics.png"></p>
</section>
<section id="結果還差強人意" class="slide level2">
<h2>結果還差強人意</h2>

<img data-src="iol1-new.png" class="r-stretch"></section></section>
<section>
<section id="幻覺與知識阻斷" class="title-slide slide level1">
<h1>幻覺與知識阻斷</h1>
<p>hallucination and knowledge-cutoff</p>
<div class="fragment">
<ul>
<li>事實幻覺（也許還好解決）</li>
</ul>
</div>
<div class="fragment">
<p><img data-src="hallucination.png" style="width:60.0%"></p>
</div>
</section>
<section id="推理判斷與假說幻覺則很棘手" class="slide level2">
<h2>推理、判斷與假說幻覺則很棘手</h2>
<div class="fragment">
<p>BART’s hallucination (每一家的幻覺都蠻嚴重的)</p>
</div>
<div class="fragment">
<p><img data-src="oma.png"></p>
</div>
<div class="fragment">
<p><span class="small-text">可愛的錯誤是沒關係，但在重要的決策（如：法律親屬繼承關係）就出大事</span></p>
</div>
</section>
<section id="section-1" class="slide level2">
<h2></h2>
<div class="callout callout-tip callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>持續學習</strong></p>
</div>
<div class="callout-content">
<p>Life-long learning for Human and Machine</p>
</div>
</div>
</div>
<ul>
<li>一個需要回答的技術哲學的問題：我們期待一個通才的 AI 還是專才的 AI 們？ (<span class="small-text">可以用我們現在期待的人類社會來想像</span>)</li>
</ul>
<div class="fragment">
<ul>
<li>讓機器與人類一起學習，可以協助人類發想、開創與演化。而這就是語言與知識資源教養 AI 的時代意義。</li>
</ul>
<!-- ##  -->
<!-- > Here are the images illustrating language and knowledge resources as the food for AI: -->
</div>
</section>
<section id="section-2" class="slide level2">
<h2></h2>
<div class="columns">
<div class="column" style="width:25%;">
<blockquote>
<p>Here are the images illustrating language and knowledge resources as the food for AI:</p>
</blockquote>
</div><div class="column" style="width:55%;">
<p><img data-src="ai-lr.png"></p>
</div>
</div>
</section></section>
<section>
<section id="那麼什麼是語言與知識資源" class="title-slide slide level1">
<h1>那麼，什麼是語言與知識資源</h1>
<p><strong>Language (and Knowledge) Resources</strong> in linguistics and language technology</p>
<div class="fragment">
<ul>
<li>Collection, processing and evaluation of digital form of <span id="magenta">language use</span>.</li>
</ul>
</div>
<div class="fragment">
<ul>
<li><p><span id="magenta"><strong>數據 data, 工具 tools, 經驗 advice</strong></span></p>
<ul>
<li>corpora and lexical (semantic) resources (e.g.&nbsp;<span id="d-blue">wordnet, framenet, e-Hownet, Conceptnet, BabelNet, ..</span>), …</li>
<li>tagger, parser, chunker, …</li>
<li>metadata, evaluation metrics, …..</li>
</ul></li>
</ul>
</div>
</section>
<section id="我們以詞彙網路舉例" class="slide level2">
<h2>我們以詞彙網路舉例</h2>
<p><code>WordNet</code> architecture: two core components:</p>
<ul>
<li>Synset (synonymous set)</li>
<li>Paradigmatic lexical (semantic) relations: hyponymy/hypernymy; meronymy/holonymy, etc</li>
</ul>

<!-- ## Chinese Wordnet: Brief History -->
<!-- :::: {.columns} -->
<!-- ::: {.column width="25%"} -->
<!-- ![](huang.png) -->
<!-- ::: -->
<!-- ::: {.column width="65%"} -->
<!-- - `Sinica BOW` [@huang2004sinica] (2000-2004) -->
<!-- - `Chinese Wordnet at Academia Sinica` (2005-2010) -->
<!-- - `Chinese Wordnet at NTU Taiwan` (2010-) -->
<!-- ::: -->
<!-- :::: -->
<!-- . . . -->
<!-- ::: {.callout-note appearance="simple" icon=false} -->
<!-- Note that there are more than one Chinese Wordnet. -->
<!-- ::: -->
<img data-src="wn.graph.png" class="r-stretch quarto-figure-right"></section>
<section id="chinese-wordnet" class="slide level2">
<h2>Chinese Wordnet</h2>
<ul>
<li><p>Follow PWN (in comparison with Sinica BOW)</p></li>
<li><p>Word segmentation principle <span class="citation" data-cites="chu2017mandarin">(<a href="#/reference" role="doc-biblioref" onclick="">Huang, Hsieh, and Chen 2017</a>)</span></p></li>
<li><p>Corpus-based decision</p></li>
<li><p>Manually created (sense distinction, gloss with controlled vocabulary, etc)</p></li>
</ul>

<img data-src="MandarinChineseWords.jpg" class="r-stretch"></section>
<section id="chinese-wordnet-1" class="slide level2">
<h2>Chinese Wordnet</h2>
<ul>
<li>The status quo: latest release 2022, <a href="https://lopentu.github.io/CwnWeb/">website</a></li>
</ul>

<img data-src="cwn.png" class="r-stretch quarto-figure-right"></section></section>
<section>
<section id="theories" class="title-slide slide level1">
<h1>Theories</h1>
<div class="callout callout-tip callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Some new perspectives in CWN</strong></p>
</div>
<div class="callout-content">
<p>sense granularity, relation discovery, glos and annotation in parallel</p>
</div>
</div>
</div>
</section>
<section id="distinction-of-meaning-facets-and-senses" class="slide level2">
<h2>Distinction of meaning facets and senses</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p><img data-src="co-predication.png"></p>
</div><div class="column" style="width:50%;">
<blockquote>
<p>埔里<em>種</em>的【茶】很<em>好喝</em></p>
</blockquote>
</div>
</div>
<div class="callout callout-tip callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Co-predicative Nouns</strong></p>
</div>
<div class="callout-content">
<p>a phenomenon where two or more predicates seem to require that their argument denotes different things.</p>
</div>
</div>
</div>
</section>
<section id="gloss-as-lexicographic-resources-with-add-ons-annotations" class="slide level2">
<h2>Gloss as lexicographic resources with add-ons annotations</h2>
<ul>
<li><p>Gloss (<span id="magenta">lexicographic definition</span>) is carefully controlled with limited vocabulary and <strong>lexical patterns</strong>, e.g.,</p>
<ul>
<li>Verbs with <code>VH</code> tag (i.e., stative intransitive verbs) are glossed with “形容 or 比喻形容 …”.</li>
<li>Adverbs are glossed with “表…”</li>
</ul></li>
<li><p>collocational information, pragmatic information (‘tone’, etc) are recorded as additional annotation.</p></li>
</ul>

<img data-src="glossStat.png" class="r-stretch quarto-figure-center"></section>
<section id="cwn-2.0-search" class="slide level2">
<h2>CWN 2.0 Search</h2>
<ul>
<li>The most comprehensive and fine-grained sense repository and network in Chinese</li>
</ul>
<div class="fragment">
<div class="quarto-figure quarto-figure-left">
<figure>
<p><img data-src="lang.png"></p>
</figure>
</div>
</div>
</section>
<section id="cwn-2.0-programmable-search" class="slide level2">
<h2>CWN 2.0 Programmable Search</h2>
<ul>
<li><a href="https://cwngraph.readthedocs.io/en/latest/">API and doc</a> freely available</li>
</ul>
<div class="sourceCode" id="cb1"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1"></a><span class="im">from</span> CwnGraph <span class="im">import</span> CwnImage</span>
<span id="cb1-2"><a href="#cb1-2"></a>cwn <span class="op">=</span> CwnImage.latest()</span>
<span id="cb1-3"><a href="#cb1-3"></a><span class="co"># the original base data</span></span>
<span id="cb1-4"><a href="#cb1-4"></a><span class="co"># from CwnGraph import CwnImage</span></span>
<span id="cb1-5"><a href="#cb1-5"></a><span class="co"># cwn = CwnBase()</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="zipfs-law-no-surprise" class="slide level2">
<h2>Zipf’s law (no surprise)</h2>
<ul>
<li>Most words have small number of senses (Zipf’s law)</li>
</ul>

<img data-src="comp.jpg" class="r-stretch"></section>
<section id="comparison-to-other-chinese-lexical-resources" class="slide level2">
<h2>Comparison to other Chinese lexical resources</h2>
<div class="columns fragment" data-fragment-index="0">
<div class="column" style="width:40%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="ehownet_sen.png"></p>
<figcaption>e-Hownet</figcaption>
</figure>
</div>
</div><div class="column" style="width:40%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="meng.png"></p>
<figcaption>Grand Dictionary of Ministry of Education</figcaption>
</figure>
</div>
</div><div class="column" style="width:40%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="cwn_sense.png"></p>
<figcaption>CWN20</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="data-summary-11" class="slide level2">
<h2>Data summary 1/1</h2>
<div class="columns fragment" data-fragment-index="1">
<div class="column" style="width:30%;">
<p><img data-src="cwn_sta.png" width="400"></p>
</div><div class="column" style="width:70%;">
<div class="cell" data-hash="index_cache/revealjs/fig-cwn1_2ddf5657d15735dc08b53a2fea0ceb0d">
<div class="cell-output-display">
<div id="fig-cwn1" class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="index_files/figure-revealjs/fig-cwn1-1.png" width="960"></p>
<figcaption>Figure&nbsp;1: cwn sense data summary</figcaption>
</figure>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="data-summary-22" class="slide level2">
<h2>Data summary 2/2</h2>
<p><a href="#/data-summary-22">Figure&nbsp;2</a> further demonstrates the distribution of different types of relations</p>
<div class="columns fragment" data-fragment-index="1">
<div class="column" style="width:30%;">
<p><img data-src="cwn_sta2.png"></p>
</div><div class="column" style="width:70%;">
<div class="cell" data-hash="index_cache/revealjs/fig-cwn2_557964a7de480ff9410832f606a70f1c">
<div class="cell-output-display">
<div id="fig-cwn2" class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="index_files/figure-revealjs/fig-cwn2-1.png" width="960"></p>
<figcaption>Figure&nbsp;2: cwn relation data summary</figcaption>
</figure>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="cwn-2.0" class="slide level2">
<h2>CWN 2.0</h2>
<p><strong>Visualization</strong></p>

<img data-src="cwnVis.png" class="r-stretch"></section>
<section id="cwn-2.0-1" class="slide level2">
<h2>CWN 2.0</h2>
<p><strong>sense tagger</strong></p>
<ul>
<li><p><span id="magenta">Transformer-based sense tagger</span></p></li>
<li><p>Leveraging wordnet <em>glosses</em> using <code>GlossBert</code> <span class="citation" data-cites="huang2019glossbert">(<a href="#/reference" role="doc-biblioref" onclick=""><strong>huang2019glossbert?</strong></a>)</span>, a BERT model for word sense disambiguation with gloss knowledge.</p></li>
<li><p>Our extended <code>GlossBert</code> model on CWN gloss+ SemCor reports 82% accuracy.</p></li>
</ul>
<aside class="notes">
<p>conducting <em>context-gloss</em> pairs, and fine-tune the pre-trained BERT model on SemCor3.0 training corpus, and achieves SOTA performance on several English all-words WSD tasks.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>

<img src="glossB.png" alt="drawing" style="width:50px;" class="r-stretch"></section>
<section id="word-sense-tagger" class="slide level2">
<h2>Word Sense Tagger</h2>
<ul>
<li>APIs (GlossBert version) released in 2021<br>
<img src="tagger.png" alt="drawing" style="width:400px;"></li>
</ul>
<div class="cell">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1"></a><span class="co"># pip install -U DistilTag SenseTagger</span></span>
<span id="cb2-2"><a href="#cb2-2"></a><span class="im">import</span> DistilTag</span>
<span id="cb2-3"><a href="#cb2-3"></a><span class="im">import</span> CwnSenseTagger</span>
<span id="cb2-4"><a href="#cb2-4"></a>DistilTag.download()</span>
<span id="cb2-5"><a href="#cb2-5"></a>CwnSenseTagger.download()</span>
<span id="cb2-6"><a href="#cb2-6"></a></span>
<span id="cb2-7"><a href="#cb2-7"></a>tagger <span class="op">=</span> DistilTag()</span>
<span id="cb2-8"><a href="#cb2-8"></a>tagged <span class="op">=</span> tagger.tag(<span class="st">"&lt;raw text&gt;"</span>)</span>
<span id="cb2-9"><a href="#cb2-9"></a>sense_tagged <span class="op">=</span> senseTag(tagged)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="senseTag.png"></p>
</figure>
</div>
</section>
<section id="cwn-2.0-2" class="slide level2">
<h2>CWN 2.0</h2>
<p><strong>Chinese SemCor</strong></p>
<ul>
<li>semi-automatically curated sense-tagged corpus based on Academic Sinica Balanced Corpus (ASBC) s.</li>
</ul>

<img data-src="senseanno.png" style="width:80.0%" class="r-stretch"></section>
<section id="cwn-based-applications" class="slide level2">
<h2>CWN-based applications</h2>
<p><strong>sense frequency distribution</strong> in corpus</p>
<ul>
<li>Now we have chance to <em>empirically</em> explore the <strong>dominance</strong> of word senses in language use, which is essential for both lexical semantic and psycholinguistic studies.</li>
</ul>
<div class="fragment">
<ul>
<li>e.g., ‘開’ (kai1,‘open’) has (surprisingly) more dominant <em>blossom</em> sense over others (based on randomly chosen 300 sentences in ASBC corpus)</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="senseFreq.png"></p>
</figure>
</div>
<aside class="notes">
<p>從平衡語料庫中取出 300 句包含「吃」或「開」的句子（一句可能含多個目標詞），用CwnSenseTagger 判斷該詞在脈絡中的詞意，並計算其頻率。以下列出頻率最高的 5 個詞意。</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</div>
</section>
<section id="cwn-based-applications-1" class="slide level2">
<h2>CWN-based applications</h2>
<p><strong>word sense acquisition</strong></p>

<img data-src="huan.png" class="r-stretch quarto-figure-center"><p class="caption">credit:郭懷元同學</p></section>
<section id="cwn-based-applications-2" class="slide level2">
<h2>CWN-based applications</h2>
<p><strong>tracking sense evolution</strong></p>
<ul>
<li>The indeterminate nature of Chinese affixoids</li>
<li>Sense status of 家 jiā from the Tang dynasty to the 1980s</li>
</ul>

<img data-src="jia_en.png" class="r-stretch quarto-figure-center"></section></section>
<section id="台灣多模態語料庫" class="title-slide slide level1">
<h1><a href="https://multimoco.linguistics.ntu.edu.tw/index.html">台灣多模態語料庫</a></h1>

<img data-src="moco.png" class="r-stretch"></section>

<section>
<section id="回到我們要探究的問題" class="title-slide slide level1">
<h1>回到我們要探究的問題</h1>
<p><code>(Autoregressive) LLMs 與 LR/KR 的關係？</code></p>
<!-- - Autoregressive (AR) models, like GPT-3 (and later versions), generate sentences word by word from left to right. They predict the next word in a sentence given all the previous words.  -->
<!-- . . . -->
<!-- - Autoencoding (AE) models, like BERT, instead focus on understanding the sentence as a whole. They are trained to fill in gaps in a sentence. -->
</section>
<section id="llm-based-nlp-a-new-paradigm" class="slide level2">
<h2>LLM-based NLP: a new paradigm</h2>
<p>Pre-train, Prompt, and Predict <span class="citation" data-cites="liu2021pretrain">(<a href="#/reference" role="doc-biblioref" onclick="">Liu et al. 2021</a>)</span></p>

<!-- ## Foundation Models -->
<!-- From LLMs to FMs (thus [Generative AI]{#magenta}) -->
<!-- ![](foundation_models.png) -->
<!-- A foundation model can centralize the information from all the data from various modalities. This -->
<!-- one model can then be adapted to a wide range of downstream tasks.[@bommasani2021opportunities] -->
<!-- ##  -->
<!-- ::: {.callout-note appearance="default"} -->
<!-- I'd still call it LLMs because of the use of NL -->
<!-- ::: -->
<!-- ## LLM-based NLP  -->
<!-- easier version -->
<!-- :::: {.columns .fragment fragment-index=1} -->
<!-- ::: {.column width="50%"} -->
<!-- ![](nlp-1.png){width="600"} -->
<!-- ![](nlp-2.png){width="600"} -->
<!-- ::: -->
<!-- ::: {.column width="50%"} -->
<!-- ![](nlp-3.png){width="600"} -->
<!-- ::: -->
<!-- :::: -->
<img data-src="nlp-paradigm.png" class="r-stretch quarto-figure-center"><p class="caption">Four paradims in NLP</p></section>
<section id="in-context-learning" class="slide level2">
<h2>In-context Learning</h2>
<blockquote>
<p>refers to methods for how to communicate with LLM to steer its behavior for desired outcomes without updating the model weights.</p>
</blockquote>
<!-- LLM is a general store of information, and in-context learning allows us to access certain parts within this store without having to specialize the entire model towards each task. -->
<ul>
<li>Amazing performance with only zero/few-shot</li>
<li>Requires no parameter updates, just talk to them in <span id="magenta">natural language</span>!</li>
<li>Prompt engineering: the process of creating a prompt that results in the most effective performance on the downstream task</li>
</ul>
</section>
<section id="prompt-and-prompt-engineering" class="slide level2">
<h2>Prompt and Prompt engineering</h2>
<div class="callout callout-note callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Note</strong></p>
</div>
<div class="callout-content">
<p><strong>prompt</strong>: a natural language description of the task.</p>
<p><strong>prompt design</strong>: involves instructions and context passed to the LLM to achieve a desired task.</p>
<p><strong>prompt engineering</strong>: the practice of developing optimal (clear, concise, informative) prompts to efficiently use LLMs for a variety of applications.</p>
</div>
</div>
</div>
<aside class="notes">
<p>Prompt Engineering is the way in which instruction and reference data is presented to the LLM.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
<div class="fragment">
<p><img data-src="prompt_de.png" style="width:40.0%"></p>
</div>
<div class="fragment">
<blockquote>
<p>(Text) Generation is a meta capability of Large Language Models &amp; Prompt Engineering is the key to unlocking it.</p>
</blockquote>
</div>
</section>
<section id="prompt-and-prompt-engineering-1" class="slide level2">
<h2>Prompt and Prompt engineering</h2>
<p><code>basic elements</code></p>
<div class="fragment">
<p>A prompt is composed with the following components:</p>
<p><img data-src="prompt_elements.png" style="width:70.0%"></p>
<p><a href="www.promptingguide.ai/">source</a></p>
</div>
</section>
<section id="prompt-and-prompt-engineering-2" class="slide level2">
<h2>Prompt and Prompt engineering</h2>
<p><code>zero and few shot</code> <span class="citation" data-cites="weng2023prompt">(<a href="#/reference" role="doc-biblioref" onclick="">Weng 2023</a>)</span> . . .</p>
<ul>
<li><p>Zero shot learning: implies that a model can recognize things that have not explicitly been taught in the training.</p></li>
<li><p>Few shot learning: refers to training a model with minimal data.</p></li>
</ul>
</section>
<section id="prompt-and-prompt-engineering-3" class="slide level2">
<h2>Prompt and Prompt engineering</h2>
<p><code>Chain-of-Thought</code> <span class="citation" data-cites="wei2023chainofthought">(<a href="#/reference" role="doc-biblioref" onclick="">Wei et al. 2023</a>)</span></p>
<ul>
<li>generates a sequence of short sentences to describe reasoning logic step by step, known as reasoning chains or rationales, to eventually lead to the final answer.</li>
</ul>
<p><img data-src="cot.png"><a href="https://youtu.be/zizonToFXDs">source</a></p>
<ul>
<li>zero or few shot CoT</li>
</ul>
<!-- ## Prompt and Prompt engineering -->
<!-- ``typology`[@weng2023prompt]` -->
<!-- - **Instruction prompting** -->
</section>
<section id="prompt-and-prompt-engineering-4" class="slide level2">
<h2>Prompt and Prompt engineering</h2>
<p><code>Self-consistency sampling</code> <span class="citation" data-cites="weng2023prompt">(<a href="#/reference" role="doc-biblioref" onclick="">Weng 2023</a>)</span></p>
<ul>
<li>to sample multiple outputs with temperature &gt; 0 and then selecting the best one out of these candidates. The criteria for selecting the best candidate can vary from task to task. A general solution is to pick majority vote.</li>
</ul>
<p>新發展可以參見 <a href="https://www.promptingguide.ai/">Prompting Guide</a></p>
</section>
<section id="prompt-and-prompt-engineering-5" class="slide level2">
<h2>Prompt and Prompt engineering</h2>
<p><code>Persona</code> setting is also important, <em>socio-linguistically</em></p>
<div class="fragment">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="pe1.png" style="width:75.0%"></p>
<figcaption>awesome-chatgpt-prompts</figcaption>
</figure>
</div>
</div>
</section>
<section id="prompting-llm-for-lexical-semantic-tasks" class="slide level2">
<h2>Prompting LLM for lexical semantic tasks</h2>
<p><code>word sense disambiguation</code></p>
<div class="fragment">
<p><img data-src="wsd.jpeg" style="width:60.0%"></p>
</div>
</section>
<section id="prompting-llm-for-solve-lexical-semantic-tasks" class="slide level2">
<h2>Prompting LLM for solve lexical semantic tasks</h2>
<p><code>sense to action</code></p>
<div class="fragment">
<p><img data-src="sense2action.jpeg"></p>
</div>
</section>
<section id="prompting-llm-for-solve-lexical-semantic-tasks-1" class="slide level2">
<h2>Prompting LLM for solve lexical semantic tasks</h2>
<p><code>word sense induction</code></p>
<div class="fragment">
<p><img data-src="sense%20induction.jpeg" style="width:70.0%"></p>
</div>
</section>
<section id="prompting-llm-for-solve-lexical-semantic-tasks-2" class="slide level2">
<h2>Prompting LLM for solve lexical semantic tasks</h2>
<p><code>code-switching wsd</code></p>
<div class="fragment">
<p><img data-src="codeswitch.png" style="width:60.0%"></p>
</div>
</section>
<section id="section-3" class="slide level2">
<h2></h2>

<img data-src="codeswitch2.png" style="width:60.0%" class="r-stretch"></section>
<section id="prompting-llm-for-wordnet-data-augmentation" class="slide level2">
<h2>Prompting LLM for Wordnet data augmentation</h2>
<p><strong>Exploratory Prompting Analysis</strong></p>
<ul>
<li>Prompting LLM becomes an empirical work and the effect of prompt engineering methods can vary a lot among models and tasks, thus requiring heavy experimentation and heuristics.</li>
</ul>
<div class="fragment">
<ul>
<li>After exploring different prompt templates, we take a prompt template with bullet points, a sequence of instructions, and a guided inquiring style to be complied with, with the persona setting as ““You are a linguist mastering in lexical semantics and in constructing Chinese Wordnet.”</li>
</ul>
</div>
</section>
<section id="section-4" class="slide level2">
<h2></h2>

<img data-src="cwn_aug_prompt.png" class="r-stretch"></section>
<section id="results" class="slide level2">
<h2>Results</h2>
<p><span id="magenta">instruction tuning</span></p>

<img data-src="cwn-aug2.png" class="r-stretch"></section>
<section id="human-ratings" class="slide level2">
<h2>Human ratings</h2>
<ul>
<li><p>Human rating is based on the word’s appropriateness of interpretation, the meanings’ correspondence to the word’s part of speech, their avoidance of oversimplification/overgeneralization, and their compliance with the prompt’s requirements.</p></li>
<li><p>The top 600 frequent words are rated to further analyze their error types.</p></li>
</ul>

<img data-src="rating.png" style="width:40.0%" class="r-stretch"></section>
<section id="prompting-limitations" class="slide level2">
<h2>Prompting limitations</h2>
<p>想辦法問博學者 savant 的各種技能，也會有天花板</p>
<blockquote>
<p><code>In-context learning (~ prompting)</code>involves providing input to the language model in a specific format to elicit the desired output.</p>
</blockquote>
<ul>
<li>提示詞脈絡視窗大小 <code>context window size</code> restricts the model’s ability to process long sequences of information effectively.</li>
</ul>
<div class="fragment">
<ul>
<li><p>各種幻覺與執著 <code>Hallucination</code> appears when the generated content is nonsensical or unfaithful to the provided source content.</p>
<ul>
<li><p>reluctance to express uncertainty, to change premise, and yields to authority</p></li>
<li><p>incapable of accessing factual/up-to-date information; or no authentic sources provided</p></li>
</ul></li>
</ul>
</div>
</section>
<section id="section-5" class="slide level2">
<h2></h2>
<ul>
<li>數據也有可能受到版權、個資與企業隱私問題。</li>
</ul>

<img data-src="llm_freq.png" style="width:10.0%" class="r-stretch"></section></section>
<section id="a-neural-symbolic-approach-to-rebuild-the-lr-ecosystem" class="title-slide slide level1">
<h1>A neural-symbolic approach to rebuild the LR ecosystem</h1>
<p><span id="magenta">Toward a more linguistic knowledge-aware LLMs</span></p>
<div class="columns fragment" data-fragment-index="4">
<div class="column" style="width:50%;">
<ul>
<li>The neural-symbolic approach seeks to integrate these two paradigms to leverage the strengths of both: the learning and generalization capabilities of neural networks and the interpretability and reasoning capabilities of symbolic systems.</li>
</ul>
</div><!--     - Grounded generation (Indexing LRs): a form of retrieval augmented generation, the LLM provides answers to user queries based not only on the knowledge captured in the dataset the LLM was trained on, but augmented with knowledge coming from searching additional data sources: --><!--     - LLM-LR app stores: LR and LLM `langchain`ed could be mutually beneficial for LLM and LR. --><!-- - Using cwn and corpus as a case study is a good start. --><div class="column" style="width:40%;">
<p><img data-src="augmented-llms.png"></p>
</div>
</div>
</section>

<section id="目前兩種作法" class="title-slide slide level1">
<h1>目前兩種作法</h1>
<ul>
<li><strong>Fine-tuning</strong> on up-to-dated / customized data</li>
</ul>
<div class="fragment">
<ul>
<li><strong>Retrieval-augmented Generation</strong> (e.g., RAG prompting)</li>
</ul>
</div>
</section>

<section>
<section id="fine-tune" class="title-slide slide level1">
<h1>Fine-tune</h1>
<p>模型的壓縮技術 quantization (<code>LoRA</code>, <code>QLoRA</code>, …) 使得微調大型語言模型變得更為可行</p>
</section>
<section id="lollama-a-fine-tuned-model" class="slide level2">
<h2>LoLlama: a fine-tuned model</h2>
<ul>
<li>We fine-tune <code>LoLlama</code> on top of Taiwan-LLaMa (Lin and Chen, 2023), which was pre-trained on over 5 billion tokens of Traditional Chinese. The model was further fine-tuned on over 490K multi-turn con- versational data to enable instruction-following and context-aware responses.</li>
<li>We train LoLlama with CWN</li>
</ul>
</section>
<section id="evaluation" class="slide level2">
<h2>Evaluation</h2>

<img data-src="lollama.png" class="r-stretch"></section>
<section id="evaluation-1" class="slide level2">
<h2>Evaluation</h2>

<img data-src="lollama2.png" class="r-stretch"></section>
<section id="問題限制" class="slide level2">
<h2>問題限制</h2>
<ul>
<li><p>商用好，但很貴，也不保證安全。</p></li>
<li><p>開源的 llm 越來越好，壓縮技術越見成熟。但訓練不便宜，動輒政治價值審查。</p></li>
</ul>
<div class="fragment">
<p><span class="small-text">（抱怨：又要馬兒跑，又要馬兒沒草吃）</span></p>
</div>
</section></section>
<section>
<section id="rag" class="title-slide slide level1">
<h1>RAG</h1>
<blockquote>
<p>Retrieving facts from an external knowledge base to ground large language models (LLMs) on the most accurate, up-to-date information and to give users insight into LLMs’ generative process.</p>
</blockquote>
<div class="fragment">
<p><img data-src="rag.png"></p>
</div>
</section>
<section id="vector-database-and-embeddings" class="slide level2">
<h2>Vector DataBase and Embeddings</h2>
<ul>
<li>A (vector) embedding is the internal representation of input data in a deep learning model, also known as embedding models or a deep neural network.</li>
</ul>

<img data-src="vector.png" class="r-stretch"><ul>
<li><p>We obtain vectors by removing the last layer and taking the output from the second-to-last layer.</p></li>
<li><p>Vector DB/ Vector Store:</p></li>
</ul>
<!-- ## LLM apps (`Everything app` v.2)  -->
<!-- - `Everything app` (aka super app) is a commercial scenario where all the living services are provided. -->
<!--   - e.g., imaging a super app (like `WeChat`) offers food delivery, ride-hailing, on-demand package delivery, financial and investing, and other services. -->
<!-- . . . -->
<!-- - Similarly, the (external knowledge and factual resources) plugins enable LLMs (like `GPT-4`, `PaLM-2`) to gather real-time information pertinent to their conversation prompts, effectively overcoming the limitation. -->
<!-- ##  LLM apps: another example -->
<!-- `JARVIS` -->
<!-- ![HuggingGPT: workflow](hugginggpt_workflow.png) -->
<!-- ##  -->
<!-- - an LLM as the controller and numerous expert models as collaborative executors (from HuggingFace Hub), demo at https://huggingface.co/spaces/microsoft/HuggingGPT -->
<!-- ![](hf_jarvis.jpeg) -->
</section>
<section id="workflow" class="slide level2">
<h2>Workflow</h2>

<img data-src="langchain-step.png" class="r-stretch"></section></section>
<section>
<section id="lopegpt-a-rag-model" class="title-slide slide level1">
<h1><span id="magenta">lopeGPT</span>: <a href="https://lope.linguistics.ntu.edu.tw/lopeGPT">a RAG model</a></h1>
<div class="columns">
<div class="column" style="width:50%;">
<!-- <img src="lopegpt.png" alt="drawing" align = "center" style="width:900px;"/> -->
<p>a higer archtecture <img data-src="lopegpt.png" width="920"></p>
</div><div class="column" style="width:50%;">
<ul>
<li>Integration of language resources:
<ul>
<li>Academia Sinica Balanced Corpus of Modern Chinese (ASBC)</li>
<li>Social Media Corpus in Taiwan (SoMe)</li>
<li>Chinese Wordnet 2.0 (CWN)</li>
</ul></li>
</ul>
<p><img data-src="lopegpt-rag.png"> <br> <a href="">https://lope.linguistics.ntu.edu.tw/lopeGPT</a></p>
</div>
</div>
</section>
<section id="experiments-on-sense-computing-tasks" class="slide level2">
<h2>Experiments on Sense Computing Tasks</h2>
<p><code>augmented LLMs</code></p>
<ul>
<li>sense definition, lexical relation query and processing</li>
<li>sense tagging and analysis</li>
</ul>
</section>
<section id="section-6" class="slide level2">
<h2></h2>

<img data-src="lopegpt-3.png" class="r-stretch"></section>
<section id="section-7" class="slide level2">
<h2></h2>

<img data-src="lopegpt-2.png" class="r-stretch"></section>
<section id="experiments-on-sense-computing-tasks-1" class="slide level2">
<h2>Experiments on Sense computing tasks</h2>
<p><code>localized and customized</code></p>
<ul>
<li><p>upload data to calculate (word frequency, word sense frequency, etc) via <code>llama-index</code> data loader.</p></li>
<li><p>vectorized the data and semantically search/compare</p></li>
<li><p>given few shot, predict the sense and automatically generate the gloss (and relations to others)</p></li>
</ul>
<div class="fragment">
<div class="callout callout-note callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Note</strong></p>
</div>
<div class="callout-content">
<p>All examples are tested with <code>chatgpt-3.5-turbo</code> (using OpenAI’s API) . It uses the default configurations, i.e., <code>temperature</code>=0.0.</p>
</div>
</div>
</div>
</div>
</section>
<section id="some-prelimenary-results" class="slide level2">
<h2>Some prelimenary results</h2>

<img data-src="lopegpt_upload.png" class="r-stretch"></section></section>
<section>
<section id="llms-orchestration" class="title-slide slide level1">
<h1>LLMs orchestration</h1>
<p>Orchestration frameworks provide a way to manage and control LLMs.</p>
<p>需要一名專案經理，協調所有進行中的工作項目，確保各專案達成理想的最終結果</p>
</section>
<section id="llms-orchestration-frameworks" class="slide level2">
<h2>LLMs orchestration frameworks</h2>
<ul>
<li><p>LlamaIndex</p></li>
<li><p>Semantic Kernel</p></li>
<li><p>LangChain</p></li>
</ul>
</section>
<section id="comparison" class="slide level2">
<h2>Comparison</h2>

<img data-src="sk.png" class="r-stretch"></section>
<section id="langchain" class="slide level2">
<h2>Langchain</h2>
<blockquote>
<p><code>langchain</code> is an open source framework that allows AI developers to combine LLMs like GPT-4 with external sources of computation and data.</p>
</blockquote>

<img data-src="star.png" style="width:50.0%" class="r-stretch quarto-figure-center"><p class="caption">Github repo star history</p></section>
<section id="architecture" class="slide level2">
<h2>Architecture</h2>

<img data-src="lanchain.gif" class="r-stretch"></section>
<section id="langchain-components" class="slide level2">
<h2>langchain components</h2>
<ul>
<li><p><strong>Chains</strong>: The core of <code>langchain</code>. Components (and even other chains) can be stringed together to create <em>chains</em>.</p></li>
<li><p><strong>Prompt templates</strong>: Prompt templates are templates for different types of prompts. Like “chatbot” style templates, ELI5 question-answering, etc</p></li>
<li><p><strong>LLMs</strong>: Large language models</p></li>
<li><p><strong>Indexing Utils:</strong> Ways to interact with specific data (embeddings, vectorstores, document loaders)</p></li>
<li><p><strong>Tools:</strong> Ways to interact with the outside world (search, calculators, etc)</p></li>
<li><p><span id="magenta"><strong>Agents</strong>: Agents use LLMs to decide what actions should be taken</span>. Tools like web search or calculators can be used, and all are packaged into a logical loop of operations.</p></li>
<li><p><strong>Memory</strong>: Short-term memory, long-term memory.</p></li>
</ul>
</section>
<section id="working-with-llms-and-your-own-data" class="slide level2">
<h2>Working with LLMs and your own data</h2>
<ul>
<li>Good news for Language and Knowledge Resource developers</li>
</ul>
<!-- Be data-aware: connect a language model to other sources of data -->
<!-- Be agentic: allow a language model to interact with its environment -->
<div class="fragment">
<p><img data-src="langchain.png" style="width:60.0%"></p>
<!-- ## Chain -->
<!-- 有點像組合函數，裏面是 prompt template, 外面是 llm -->
<!-- ## Next:  -->
<!-- 如此一來，Auto-GPT 就能打造出一個閉環（closed loop），進而有能力應對更複雜的多步程序；我們也可以把 Auto-GPT 看作一名專案經理，協調所有進行中的工作項目，確保各專案達成理想的最終結果。目前 Auto-GPT 能夠與線上和本地的應用程序、軟體和服務進行各種互動，例如網頁瀏覽器與文字處理器。 . . . -->
<!-- Once you have explored and want to bring it into memory, it's also fast! -->
<!-- # Final thoughts -->
<!-- . . . -->
<!-- - Democratizing LLM and connect LR help keep balance between the generalization capabilities of neural networks and the interpretability and reasoning capabilities of symbolic knowledge systems.  -->
<!--     - LLM chains; Agent network -->
<!-- . . . -->
<!-- - [Auto-GPT]{#magenta} and alike as Project Manager (for a scaled and manageable ecosystem for `LR + LLM` based applications) -->
<!-- . . . -->

<!-- > More and more democratized LLMs and more diverse ecosystems which lead to a more stable symbiosis for Human and Machine. -->
<!-- # Acknowledgment -->
<!-- > 連大成、陳品而、王伯雅、古貿昌、張淳涵以及 lopers 們！ -->
</div>
</section>
<section id="總結" class="slide level2">
<h2>總結</h2>
<ul>
<li>語言與知識資源與模型訓練一樣重要：可信任、可解釋、可克制（與客製）。</li>
</ul>
<div class="fragment">
<ul>
<li>兩者連結的可能目前是 Fine-tune 與 RAG prompting。</li>
</ul>
</div>
<div class="fragment">
<ul>
<li>了解 orchestration 的架構 (e.g.，<code>langchain</code>) 對於部署 LLM 應用變成核心技能。</li>
</ul>
</div>
</section>
<section id="reference" class="slide level2 smaller scrollable">
<h2>Reference</h2>
<div id="refs" class="references csl-bib-body hanging-indent" role="list">
<div id="ref-chu2017mandarin" class="csl-entry" role="listitem">
Huang, Chu-Ren, Shu-Kai Hsieh, and Keh-Jiann Chen. 2017. <em>Mandarin Chinese Words and Parts of Speech: A Corpus-Based Study</em>. Routledge.
</div>
<div id="ref-liu2021pretrain" class="csl-entry" role="listitem">
Liu, Pengfei, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. 2021. <span>“Pre-Train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing.”</span> <a href="https://arxiv.org/abs/2107.13586">https://arxiv.org/abs/2107.13586</a>.
</div>
<div id="ref-wei2023chainofthought" class="csl-entry" role="listitem">
Wei, Jason, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. 2023. <span>“Chain-of-Thought Prompting Elicits Reasoning in Large Language Models.”</span> <a href="https://arxiv.org/abs/2201.11903">https://arxiv.org/abs/2201.11903</a>.
</div>
<div id="ref-weng2023prompt" class="csl-entry" role="listitem">
Weng, Lilian. 2023. <span>“Prompt Engineering.”</span> <em>Lilianweng.github.io</em>, March. <a href="https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/">https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/</a>.
</div>
</div>
<!-- # [祝黃老師 福如東海 壽比南山]{#magenta} -->
<!-- <iframe data-external="1" src="end.mp4" width="100%" height="85%"></iframe> -->
<!-- {{< "video" "end.mp4" "width"="100%" "height"="85%" >}} -->
</section>
<section id="hands-on-coding-tutorial" class="slide level2">
<h2>Hands-on coding tutorial</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p><img data-src="langchain-tutorial.png" style="width:35.0%"></p>
<p><br> <a href="https://t.ly/p3Z4_">colab notebook 連結</a></p>
</div><div class="column" style="width:50%;">
<p><img data-src="piner.jpg" width="200"></p>
<p><img data-src="richard.jpg"></p>
</div>
</div>
<p><img src="https://lope.linguistics.ntu.edu.tw/static/media/logo_lope.e082917e.jpg" class="slide-logo"></p>
<div class="footer footer-default">

</div>
</section></section>
    </div>
  </div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="index_files/libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="index_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="index_files/libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="index_files/libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="index_files/libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="index_files/libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="index_files/libs/revealjs/plugin/notes/notes.js"></script>
  <script src="index_files/libs/revealjs/plugin/search/search.js"></script>
  <script src="index_files/libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="index_files/libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': true,
'previewLinksAuto': false,
'smaller': false,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: false,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'none',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1920,

        height: 1080,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        math: {
          mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, QuartoSupport,

          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    
    <script>
      // htmlwidgets need to know to resize themselves when slides are shown/hidden.
      // Fire the "slideenter" event (handled by htmlwidgets.js) when the current
      // slide changes (different for each slide format).
      (function () {
        // dispatch for htmlwidgets
        function fireSlideEnter() {
          const event = window.document.createEvent("Event");
          event.initEvent("slideenter", true, true);
          window.document.dispatchEvent(event);
        }

        function fireSlideChanged(previousSlide, currentSlide) {
          fireSlideEnter();

          // dispatch for shiny
          if (window.jQuery) {
            if (previousSlide) {
              window.jQuery(previousSlide).trigger("hidden");
            }
            if (currentSlide) {
              window.jQuery(currentSlide).trigger("shown");
            }
          }
        }

        // hookup for slidy
        if (window.w3c_slidy) {
          window.w3c_slidy.add_observer(function (slide_num) {
            // slide_num starts at position 1
            fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);
          });
        }

      })();
    </script>

    <script id="quarto-html-after-body" type="application/javascript">
    window.document.addEventListener("DOMContentLoaded", function (event) {
      const toggleBodyColorMode = (bsSheetEl) => {
        const mode = bsSheetEl.getAttribute("data-mode");
        const bodyEl = window.document.querySelector("body");
        if (mode === "dark") {
          bodyEl.classList.add("quarto-dark");
          bodyEl.classList.remove("quarto-light");
        } else {
          bodyEl.classList.add("quarto-light");
          bodyEl.classList.remove("quarto-dark");
        }
      }
      const toggleBodyColorPrimary = () => {
        const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
        if (bsSheetEl) {
          toggleBodyColorMode(bsSheetEl);
        }
      }
      toggleBodyColorPrimary();  
      const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
      tabsets.forEach(function(tabset) {
        const tabby = new Tabby('#' + tabset.id);
      });
      const isCodeAnnotation = (el) => {
        for (const clz of el.classList) {
          if (clz.startsWith('code-annotation-')) {                     
            return true;
          }
        }
        return false;
      }
      const clipboard = new window.ClipboardJS('.code-copy-button', {
        text: function(trigger) {
          const codeEl = trigger.previousElementSibling.cloneNode(true);
          for (const childEl of codeEl.children) {
            if (isCodeAnnotation(childEl)) {
              childEl.remove();
            }
          }
          return codeEl.innerText;
        }
      });
      clipboard.on('success', function(e) {
        // button target
        const button = e.trigger;
        // don't keep focus
        button.blur();
        // flash "checked"
        button.classList.add('code-copy-button-checked');
        var currentTitle = button.getAttribute("title");
        button.setAttribute("title", "Copied!");
        let tooltip;
        if (window.bootstrap) {
          button.setAttribute("data-bs-toggle", "tooltip");
          button.setAttribute("data-bs-placement", "left");
          button.setAttribute("data-bs-title", "Copied!");
          tooltip = new bootstrap.Tooltip(button, 
            { trigger: "manual", 
              customClass: "code-copy-button-tooltip",
              offset: [0, -8]});
          tooltip.show();    
        }
        setTimeout(function() {
          if (tooltip) {
            tooltip.hide();
            button.removeAttribute("data-bs-title");
            button.removeAttribute("data-bs-toggle");
            button.removeAttribute("data-bs-placement");
          }
          button.setAttribute("title", currentTitle);
          button.classList.remove('code-copy-button-checked');
        }, 1000);
        // clear code selection
        e.clearSelection();
      });
      function tippyHover(el, contentFn) {
        const config = {
          allowHTML: true,
          content: contentFn,
          maxWidth: 500,
          delay: 100,
          arrow: false,
          appendTo: function(el) {
              return el.closest('section.slide') || el.parentElement;
          },
          interactive: true,
          interactiveBorder: 10,
          theme: 'light-border',
          placement: 'bottom-start'
        };
          config['offset'] = [0,0];
          config['maxWidth'] = 700;
        window.tippy(el, config); 
      }
      const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
      for (var i=0; i<noterefs.length; i++) {
        const ref = noterefs[i];
        tippyHover(ref, function() {
          // use id or data attribute instead here
          let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
          try { href = new URL(href).hash; } catch {}
          const id = href.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          return note.innerHTML;
        });
      }
      const findCites = (el) => {
        const parentEl = el.parentElement;
        if (parentEl) {
          const cites = parentEl.dataset.cites;
          if (cites) {
            return {
              el,
              cites: cites.split(' ')
            };
          } else {
            return findCites(el.parentElement)
          }
        } else {
          return undefined;
        }
      };
      var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
      for (var i=0; i<bibliorefs.length; i++) {
        const ref = bibliorefs[i];
        const citeInfo = findCites(ref);
        if (citeInfo) {
          tippyHover(citeInfo.el, function() {
            var popup = window.document.createElement('div');
            citeInfo.cites.forEach(function(cite) {
              var citeDiv = window.document.createElement('div');
              citeDiv.classList.add('hanging-indent');
              citeDiv.classList.add('csl-entry');
              var biblioDiv = window.document.getElementById('ref-' + cite);
              if (biblioDiv) {
                citeDiv.innerHTML = biblioDiv.innerHTML;
              }
              popup.appendChild(citeDiv);
            });
            return popup.innerHTML;
          });
        }
      }
    });
    </script>
    

</body></html>