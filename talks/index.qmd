---
format: 
  revealjs:
    theme: ["theme/q-theme.scss"]
    slide-number: c/t
    logo: "https://lope.linguistics.ntu.edu.tw/static/media/logo_lope.e082917e.jpg"
    footer: "[CWN2.0 ::: lopeGPT](https://lopentu.github.io/CwnWeb/)"
    code-copy: true
    center-title-slide: false
    include-in-header: heading-meta.html
    code-link: true
    code-overflow: wrap
    highlight-style: a11y
    height: 1080
    width: 1920
    toc: false
    toc-title: Contents
    toc-depth: 1
    toc_float: true
    toc-location: left # default is right, option is 'body'
execute: 
  eval: true
  echo: true
bibliography: cllt_v2.bib
---

<h1>Language Resources with/in Language Models</h1>

<h2>a case of `cwn`</h2>

<hr>

<h3>Shu-Kai Hsieh</h3>

<h3>Graduate Institute of Linguistics, National Taiwan University</h3>

<h4>2023/05/12, CBS Research Symposium, PolyU. Hong Kong</h4>

<br>

![](https://lopentu.github.io/CwnWeb/assets/img/cwn-logo-main.svg){.absolute top="600" left="1250" width="400"}

![](langchain.webp){.absolute top="800" left="950" width="550"}


# Where all starts with ..
. . . 

![](huang_cite.png){width="40%"}




# Introduction

- Language Resources 

- Large Language Models

- Proposed linkage



# Language Resources
in linguistics and language technology

- Collection, processing and evaluation of digital form of [language use]{#d-blue}.

- **data, tools, advice**

  - corpora and lexical (semantic) resources (e.g. [wordnet]{#magenta}), ...
  - tagger, parser, ...
  - metadata, evaluation metrics, .....

## WordNet architecture

Two core components:

-   Synset (synonymous set)
-   Paradigmatic lexical (semantic) relations: hyponymy/hypernymy; meronymy/holonymy, etc

![](wn.graph.png){fig-align="right"}


## Chinese Wordnet: Brief History

:::: {.columns}

::: {.column width="25%"}

![](huang.png)

:::

::: {.column width="65%"}


- `Sinica BOW` [@huang2004sinica] (2000-2004)



- `Chinese Wordnet at Academia Sinica` (2005-2010)

 

- `Chinese Wordnet at NTU Taiwan` (2010-)


:::
::::

. . .

::: {.callout-note appearance="simple" icon=false}

Note that there are more than one Chinese Wordnet.

:::


## Chinese Wordnet

-   Follow PWN (in comparison with Sinica BOW)

-   Word segmentation principle [@chu2017mandarin]

-   Corpus-based decision

-   Manually created (sense distinction, gloss with controlled vocabulary, etc)

- The status quo: latest release 2022, [website online](https://lopentu.github.io/CwnWeb/)

. . .

![](cwn.png){fig-align="right"}

# Theories

::: callout-tip
## Some new perspectives in CWN

sense granularity, relation discovery, and gloss with annotation
:::

## Meaning facets vs senses

::: columns
::: {.column width="50%"}
![](co-predication.png)
:::

::: {.column width="50%"}
> 埔里*種*的【茶】很*好喝*
:::
:::

::: callout-tip
## Co-predicative Nouns

a phenomenon where two or more predicates seem to require that their argument denotes different things.
:::


## Gloss as lexicographic resources with add-ons annotations

-   Gloss (`lexicographic definition') is carefully controlled with limited vocabulary and **patterns**, e.g.,

    -   Verbs with `VH` tag (i.e., stative intransitive verbs) are glossed with "形容 or 比喻形容 ...".
    -   Adverbs are glossed with "表..."

-   collocational information, pragmatic information ('tone', etc) are recorded as additional annotation.



![](glossStat.png){fig-align="center"}



## CWN 2.0 Search

-   The most comprehensive and fine-grained sense repository and network in Chinese

. . .

![](eat.png){fig-align="left"}

## CWN 2.0 Programmable Search

-   [API and doc](https://cwngraph.readthedocs.io/en/latest/) freely available


```python
from CwnGraph import CwnImage
cwn = CwnImage.latest()
# the original base data
# from CwnGraph import CwnImage
# cwn = CwnBase()
```

## Zipf's law (no surprise)

-   Most words have small number of senses (Zipf's law)

![](comp.jpg)



## Comparison to others


:::: {.columns .fragment fragment-index=0}


::: {.column width="40%"}

![e-Hownet](ehownet_sen.png)
:::

::: {.column width="40%"}

![MOE](meng.png)
:::

::: {.column width="40%"}
![CWN20](cwn_sense.png)
:::

::::


## Data summary 1/1


:::: {.columns .fragment fragment-index=1}

::: {.column width="30%"}
![](cwn_sta.png){width=400}
:::

::: {.column width="70%"}


```{r, echo=FALSE, cache=TRUE}
#| label: fig-cwn1
#| fig-cap: cwn sense data summary
#| warning: false
require(tibble)
require(ggplot2)
require(stringr)

df <- tribble(
  ~POS, ~count,
  "Adj", 886,
  "Adv", 1801,
  "N", 12574,
  "V", 13615,
)

ggplot(df) +
  geom_col(aes(x=reorder(POS, -count), y=count, fill=POS)) + 
  labs(x="POS", y="count") + 
  guides(fill="none") +
  geom_text(aes(x=reorder(POS, -count), y=count+400, label=count)) +
  labs(title = str_c("Total Sense Count = ", sum(df$count)))
```
:::

::::


## Data summary 2/2

@fig-cwn2 further demonstrates the distribution of different types of relations

:::: {.columns .fragment fragment-index=1}

::: {.column width="30%"}

![](cwn_sta2.png)
:::

::: {.column width="70%"}

```{r, echo=FALSE, cache=TRUE}
#| label: fig-cwn2
#| fig-cap: cwn relation data summary
#| warning: false

library(tidyverse)
library(ggplot2)

df <- read_csv("./data/cwn_relation_statistics.csv")
df %>% count(relation_type) %>% ggplot() + geom_col(aes(x=reorder(relation_type, n), y=n, fill=relation_type)) + coord_flip() + labs(y="count", x="relation") + guides(fill="none") + theme(axis.title = element_text(size = 16), axis.text = element_text(size = 14)) 
```

:::

::::




## CWN 2.0 
**Visualization**

![](cwnVis.png)


## CWN 2.0
**sense tagger**

- [Transformer-based sense tagger]{#magenta}

-   Leveraging wordnet *glosses* using `GlossBert`,a BERT model for word sense disambiguation with gloss knowledge [@huang2019glossbert].

-   Our extended `GlossBert` model on CWN gloss+ SemCor reports 82% accuracy.

::: notes
conducting *context-gloss* pairs, and fine-tune the pre-trained BERT model on SemCor3.0 training corpus, and achieves SOTA performance on several English all-words WSD tasks.
:::

<img src="glossB.png" alt="drawing" style="width:50px;"/>


## Word Sense Tagger

- APIs (GlossBert version) released in 2021\
<img src="tagger.png" alt="drawing" style="width:400px;"/>

```{python}
#| eval: false
#| echo: true
# pip install -U DistilTag SenseTagger
import DistilTag
import CwnSenseTagger
DistilTag.download()
CwnSenseTagger.download()

tagger = DistilTag()
tagged = tagger.tag("<raw text>")
sense_tagged = senseTag(tagged)
```

![](senseTag.png){fig-align="center"}


## CWN 2.0
**SemCor**

- `SemCor` (semi-automatically curated sense-tagged corpus) based on ASBC.

![](senseanno.png){fig-align="left"}




## CWN-based applications
sense frequency distribution in corpus

-   Now we have chance to *empirically* explore the **dominance** of word senses, which is essential for both lexical semantic and psycholinguistic studies.

. . .

- '開' (kai1,'open') has (surprisingly) more dominant *blossom* sense over others (based on randomly chosen 300 sentences in ASBC corpus)

![](senseFreq.png){fig-align="center"}

::: notes
從平衡語料庫中取出 300 句包含「吃」或「開」的句子（一句可能含多個目標詞），用CwnSenseTagger 判斷該詞在脈絡中的詞意，並計算其頻率。以下列出頻率最高的 5 個詞意。
:::


## CWN-based applications
**word sense acquisition**



![credit:郭懷元同學](huan.png)


## CWN-based applications
**tracking sense evolution**

-   The indeterminate nature of Chinese affixoids
-   Sense status of 家 jiā from the Tang dynasty to the 1980s

![](jia_en.png){fig-align="center"}



# Large Language Models (LLMs)

. . . 

> Large language models refer to large general-purpose language models that can be pre-trained and then fine-tuned for specific purposes.


![](llm.png)



## Large Language Models (LLMs)

- Autoregressive (AR) models, like GPT-3 (and later versions), generate sentences word by word from left to right. They predict the next word in a sentence given all the previous words. 

. . .

- Autoencoding (AE) models, like BERT, instead focus on understanding the sentence as a whole. They are trained to fill in gaps in a sentence.


## LLM-based NLP: a new paradigm
Pre-train, Prompt, and Predict [@liu2021pretrain]

![Four paradims in NLP](nlp-paradigm.png)


## Foundation Models
From LLMs to FMs (thus [Generative AI]{#magenta})

![](foundation_models.png)

A foundation model can centralize the information from all the data from various modalities. This
one model can then be adapted to a wide range of downstream tasks.[@bommasani2021opportunities]


<!-- ##  -->

<!-- ::: {.callout-note appearance="default"} -->

<!-- I'd still call it LLMs because of the use of NL -->

<!-- ::: -->



<!-- ## LLM-based NLP  -->
<!-- easier version -->

<!-- :::: {.columns .fragment fragment-index=1} -->

<!-- ::: {.column width="50%"} -->

<!-- ![](nlp-1.png){width="600"} -->


<!-- ![](nlp-2.png){width="600"} -->


<!-- ::: -->

<!-- ::: {.column width="50%"} -->

<!-- ![](nlp-3.png){width="600"} -->
<!-- ::: -->

<!-- :::: -->





## In-context Learning 

> refers to methods for how to communicate with LLM to steer its behavior for desired outcomes without updating the model weights.

<!-- LLM is a general store of information, and in-context learning allows us to access certain parts within this store without having to specialize the entire model towards each task. -->

- Amazing performance with only zero/few-shot
- Requires no parameter updates, just talk to them in [natural language]{#magenta}!
- Prompt engineering: the process of creating a prompt that results in the most effective performance on the downstream task





## Prompt and Prompt engineering

::: {.callout-note appearance="default"}

**prompt**: a natural language description of the task.

**prompt design**: involves instructions and context passed to the LLM to achieve a desired task.

**prompt engineering**: the practice of developing optimal (clear, concise, informative) prompts to efficiently use LLMs for a variety of applications.

:::

::: {.notes}
Prompt Engineering is the way in which instruction and reference data is presented to the LLM.
:::

. . . 

![](prompt_de.png){width=40%}

. . .

> (Text) Generation is a meta capability of Large Language Models & Prompt Engineering is the key to unlocking it. 



  
## Prompt and Prompt engineering
`basic elements`

. . .

A prompt is composed with the following components:


![](prompt_elements.png){width=70%}



[source](www.promptingguide.ai/)

## Prompt and Prompt engineering
`zero and few shot` [@weng2023prompt]
. . .

- Zero shot learning: implies that a model can recognize things that have not explicitly been taught in the training.

- Few shot learning: refers to training a model with minimal data.



## Prompt and Prompt engineering
`Chain-of-Thought` [@wei2023chainofthought] 

- generates a sequence of short sentences to describe reasoning logic step by step, known as reasoning chains or rationales, to eventually lead to the final answer. 

![](cot.png)[source](https://youtu.be/zizonToFXDs)

- zero or few shot CoT



<!-- ## Prompt and Prompt engineering -->
<!-- ``typology`[@weng2023prompt]` -->

<!-- - **Instruction prompting** -->



## Prompt and Prompt engineering
`Self-consistency sampling` [@weng2023prompt]

- to sample multiple outputs with temperature > 0 and then selecting the best one out of these candidates. The criteria for selecting the best candidate can vary from task to task. A general solution is to pick majority vote. 





## Prompt and Prompt engineering
`Persona` setting is also important, *socio-linguistically* 

. . .

![awesome-chatgpt-prompts](pe1.png){width=75%}




## Prompting LLM for lexical semantic tasks
`word sense disambiguation`

. . .

![](wsd.jpeg){width="60%"}

## Prompting LLM for solve lexical semantic tasks
`sense to action`

. . .

![](sense2action.jpeg)

## Prompting LLM for solve lexical semantic tasks
`word sense induction`

. . .

![](sense induction.jpeg){width=70%}

## Prompting LLM for solve lexical semantic tasks
`code-switching wsd`

. . .

![](codeswitch.png){width=60%}

## 
 ![](codeswitch2.png){width=60%}




## Prompting LLM for Wordnet data augmentation
**Exploratory Prompting Analysis**

- Prompting LLM becomes an empirical work and the effect of prompt engineering methods can vary a lot among models and tasks, thus requiring heavy experimentation and heuristics.

. . . 

- After exploring different prompt templates, we take a prompt template with bullet points, a sequence of instructions, and a guided inquiring style to be complied with, with the persona setting as "“You are a linguist mastering in lexical semantics and in constructing Chinese Wordnet.”

## 

![](cwn_aug_prompt.png)




## Results
[instruction tuning]{#magenta} 


![](cwn-aug2.png)




## Human ratings

- Human rating is based on the word's appropriateness of interpretation, the meanings' correspondence to the word's part of speech, their avoidance of oversimplification/overgeneralization, and their compliance with the prompt's requirements.

- The top 600 frequent words are rated to further analyze their error types. 


![](rating.png){width=40%}









# Limitations: Hallucination

> when the generated content is nonsensical or unfaithful to the provided source content.

. . .

![](hallucination.png){width=50%}



<!-- reluctance to express uncertainty, to change premise, and caught in a lie -->




# Limitations: Knowledge Cutoff

- incapable of accessing factual/up-to-date information 

![](llm_freq.png){width=30%}


# A neural-symbolic approach to rebuild the LR ecosystem
[Toward a more linguistic knowledge-aware LLMs]{#magenta}


- The neural-symbolic approach seeks to integrate these two paradigms to leverage the strengths of both: the learning and generalization capabilities of neural networks and the interpretability and reasoning capabilities of symbolic systems. 


:::: {.columns .fragment fragment-index=4} 

::: {.column width="50%"} 

    - Grounded generation (Indexing LRs): a form of retrieval augmented generation, the LLM provides answers to user queries based not only on the knowledge captured in the dataset the LLM was trained on, but augmented with knowledge coming from searching additional data sources:

    - LLM-LR app stores: LR and LLM `langchain`ed could be mutually beneficial for LLM and LR.

- Using cwn and corpus as a case study is a good start.

:::

::: {.column width="40%"} 


![](augmented-llms.png)
:::
::::





## LLM apps (`Everything app` v.2) 

- `Everything app` (aka super app) is a commercial scenario where all the living services are provided.
  - e.g., imaging a super app (like `WeChat`) offers food delivery, ride-hailing, on-demand package delivery, financial and investing, and other services.
  
. . .

- Similarly, the plugins enable LLMs (like `GPT-4`, `PaLM-2`) to gather real-time information pertinent to their conversation prompts, effectively overcoming the limitation.


##  LLM apps: another example
`JARVIS`

![HuggingGPT: workflow](hugginggpt_workflow.png)

## 

- an LLM as the controller and numerous expert models as collaborative executors (from HuggingFace Hub), demo at https://huggingface.co/spaces/microsoft/HuggingGPT

![](hf_jarvis.jpeg)




## Langchain

> `langchain` is an open source framework that allows AI developers to combine LLMs like GPT-4 with external sources of computation and data.

![Github repo star history](langchain_star.png){width="50%"}



## Architecture

![](langchain_components.png)

## langchain components

- **Chains**: The core of `langchain`. Components (and even other chains) can be stringed together to create *chains*.

- **Prompt templates**: Prompt templates are templates for different types of prompts. Like “chatbot” style templates, ELI5 question-answering, etc

- **LLMs**: Large language models

- **Indexing Utils:** Ways to interact with specific data (embeddings, vectorstores, document loaders)

- **Tools:** Ways to interact with the outside world (search, calculators, etc)

- **Agents**: Agents use LLMs to decide what actions should be taken. Tools like web search or calculators can be used, and all are packaged into a logical loop of operations.

- **Memory**: Short-term memory, long-term memory.




## Working with LLMs and your own data

-   Good news for Language Resource developers

<!-- Be data-aware: connect a language model to other sources of data -->
<!-- Be agentic: allow a language model to interact with its environment -->

. . .

![](langchain.png){width=40%}


<!-- ## Chain -->

<!-- 有點像組合函數，裏面是 prompt template, 外面是 llm -->



## Workflow

![](langchain-step.png)



# CWN 2.0 and Corpus plugins for LLMs
[lopeGPT]{#magenta}: high-level architecture


<!-- <img src="lopegpt.png" alt="drawing" align = "center" style="width:900px;"/> -->

![](lopegpt.png){width="920px"}



## Experiments on Sense Computing Tasks
`augmented LLMs`

- sense definition, lexical relation query and processing

- sense tagging and analysis 
標記 ...



## Experiments on Sense computing tasks
`localized and customized`

- upload data to calculate (word frequency, word sense frequency, etc) via `llama-index` data loader.

- vectorized the data and semantically search/compare 

- given few shot, predict the sense and automatically generate the gloss (and relations to others)

. . .

::: {.callout-note appearance="default"}

All examples are tested with `chatgpt-3.5-turbo` (using OpenAI's API…..) . It uses the default configurations, i.e., `temperature`=0.0.

:::


## Some prelimenary results

![](lopegpt_upload.png)




## Web app demo 



<!-- ## Next:  -->

<!-- 如此一來，Auto-GPT 就能打造出一個閉環（closed loop），進而有能力應對更複雜的多步程序；我們也可以把 Auto-GPT 看作一名專案經理，協調所有進行中的工作項目，確保各專案達成理想的最終結果。目前 Auto-GPT 能夠與線上和本地的應用程序、軟體和服務進行各種互動，例如網頁瀏覽器與文字處理器。 . . . -->

<!-- Once you have explored and want to bring it into memory, it's also fast! -->


# Final thoughts

. . .

- Democratizing LLM and connect LR help keep balance between the generalization capabilities of neural networks and the interpretability and reasoning capabilities of symbolic knowledge systems. 
    - LLM chains; Agent network

. . .

- [Auto-GPT]{#magenta} and alike as Project Manager (for a scaled and manageable ecosystem for `LR + LLM` based applications)

. . .

> More and more democratized LLMs and more diverse ecosystems which lead to a more stable symbiosis for Human and Machine.




# Acknowledgment


> 連大成、陳品而、王伯雅、古貿昌、張淳涵以及 lopers 們！


## Reference

::: {#refs}
:::



# [祝黃老師 福如東海 壽比南山]{#magenta}

<!-- <iframe data-external="1" src="end.mp4" width="100%" height="85%"></iframe> -->

<!-- {{< "video" "end.mp4" "width"="100%" "height"="85%" >}} -->


