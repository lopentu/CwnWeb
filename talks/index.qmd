---
format: 
  revealjs:
    theme: ["theme/q-theme.scss"]
    slide-number: c/t
    logo: "https://lope.linguistics.ntu.edu.tw/static/media/logo_lope.e082917e.jpg"
    # footer: "[CWN2.0 ::: lopeGPT](https://lopentu.github.io/CwnWeb/)"
    code-copy: true
    center-title-slide: false
    include-in-header: heading-meta.html
    code-link: true
    code-overflow: wrap
    highlight-style: a11y
    height: 1080
    width: 1920
    toc: false
    toc-title: Contents
    toc-depth: 1
    toc_float: true
    toc-location: left # default is right, option is 'body'
execute: 
  eval: true
  echo: true
bibliography: cllt_v2.bib
--- 

<h1>Chaining Language and Knowledge Resources with LLM(s)</h1>

<h2>a tutorial at ROCLING 2023</h2>

<hr>
<br>
<h3>Shu-Kai Hsieh 謝舒凱 </h3>

<h4>Graduate Institute of Linguistics/Brain and Mind, NTU</h4>

<br>
<h3>Piner Chen and Dachen Lian 陳品而 連大成</h3>

<h4>Graduate Institute of Linguistics, NTU</h4>

<br>

![](https://lopentu.github.io/CwnWeb/assets/img/cwn-logo-main.svg){.absolute top="600" left="1250" width="400"}

![](langchain.webp){.absolute top="800" left="950" width="550"}


<!-- # Where all starts with .. -->
<!-- . . .  -->

<!-- ![](huang_cite.png){width="40%"} -->




# 今天要談的主題

- Language Resources and Large Language Models: possible linkages

- Hands-on code session 

![](talk_qr.png){width=70}

# 背景
[Language Models]{#magenta}

. . . 

  - Probability distributions over sequences of words, tokens or characters (Shannon, 1948; 1951)

  - As a core task in NLP, often framed as **next token prediction**.


## 預訓練大型語言模型橫空出世

[Pre-trained Large Language Models]{#magenta}

. . . 

- Transformer-based pre-trained Large Language Models changed NLP/the world

![](llm_tree.png){fig-align="middle" width=50%}


##  

- **Emergent Abilities** in LLM : Unpredictable Abilities in Large Language Models
From generation to understanding?

直到大型語言模型開始出現結構理解的*頓悟*行為 (emergence)之後，
開始有推理能力的期待。


![](emergence.png){fig-align="middle" width=2%}


## Science exam reasoning

. . .

![](reasoning.png)

## [Mathematical reasoning](https://openai.com/research/formal-math)

. . .

![](reasoning2.png){width=70%}



## Iterative Reasoning and Cultural Imagination
[Rosetta Stone Problems]{#magenta}

. . . 

![](iol2.png){width=800}




## 跨符碼類型推理

![](iol1.png)
![](metrics.png)

## 結果還差強人意
![](iol1-new.png)

# 幻覺與知識阻斷
hallucination and knowledge-cutoff

. . . 

- 事實幻覺（也許還好解決）

. . .

![](hallucination.png){width=60%}


## 推理、判斷與假說幻覺則很棘手

. . . 

BART's hallucination
(每一家的幻覺都蠻嚴重的)

. . .

![](oma.png){width=60%}



##
[可愛的錯誤是沒關係，但在重要的決策（如：法律親屬繼承關係）就出大事]{.small-text}

![](bard.png){width=60%}


## 


::: callout-tip
## 持續學習
Life-long learning for Human and Machine
:::


- 一個需要回答的技術哲學的問題：我們期待一個通才的 AI 還是專才的 AI 們？
([可以用我們現在期待的人類社會來想像]{.small-text})

. . . 

- 讓機器與人類一起學習，可以協助人類發想、開創與演化。而這就是語言與知識資源教養 AI 的時代意義。


<!-- ##  -->
<!-- > Here are the images illustrating language and knowledge resources as the food for AI: -->

## 

:::: {.columns}

::: {.column width="25%"}

> Here are the images illustrating language and knowledge resources as the food for AI:

:::

::: {.column width="55%"}

![](ai-lr.png)

:::


::::

# 那麼，什麼是語言與知識資源
**Language (and Knowledge) Resources** in linguistics and language technology

. . . 

- Collection, processing and evaluation of digital form of [language use]{#magenta}.

. . .

- [**數據 data, 工具 tools, 經驗 advice**]{#magenta}

  - corpora and lexical (semantic) resources (e.g. [wordnet, framenet, e-Hownet, Conceptnet, BabelNet, ..]{#d-blue}), ...
  - tagger, parser, chunker, ...
  - metadata, evaluation metrics, .....



## 我們以詞彙網路舉例
`WordNet` architecture: two core components:

-   Synset (synonymous set)
-   Paradigmatic lexical (semantic) relations: hyponymy/hypernymy; meronymy/holonymy, etc

![](wn.graph.png){fig-align="right"}


<!-- ## Chinese Wordnet: Brief History -->

<!-- :::: {.columns} -->

<!-- ::: {.column width="25%"} -->

<!-- ![](huang.png) -->

<!-- ::: -->

<!-- ::: {.column width="65%"} -->


<!-- - `Sinica BOW` [@huang2004sinica] (2000-2004) -->



<!-- - `Chinese Wordnet at Academia Sinica` (2005-2010) -->



<!-- - `Chinese Wordnet at NTU Taiwan` (2010-) -->


<!-- ::: -->
<!-- :::: -->

<!-- . . . -->

<!-- ::: {.callout-note appearance="simple" icon=false} -->

<!-- Note that there are more than one Chinese Wordnet. -->

<!-- ::: -->


## Chinese Wordnet

-   Follow PWN (in comparison with Sinica BOW)

-   Word segmentation principle [@chu2017mandarin]

-   Corpus-based decision

-   Manually created (sense distinction, gloss with controlled vocabulary, etc)



![](MandarinChineseWords.jpg)

## Chinese Wordnet
- The status quo: latest release 2022, [website](https://lopentu.github.io/CwnWeb/)

![](cwn.png){fig-align="right"}

# Theories

::: callout-tip
## Some new perspectives in CWN

sense granularity, relation discovery, glos and annotation in parallel
:::

## Distinction of meaning facets and senses

::: columns
::: {.column width="50%"}
![](co-predication.png)
:::

::: {.column width="50%"}
> 埔里*種*的【茶】很*好喝*
:::
:::

::: callout-tip
## Co-predicative Nouns

a phenomenon where two or more predicates seem to require that their argument denotes different things.
:::


## Gloss as lexicographic resources with add-ons annotations

-   Gloss ([lexicographic definition]{#magenta}) is carefully controlled with limited vocabulary and **lexical patterns**, e.g.,

    -   Verbs with `VH` tag (i.e., stative intransitive verbs) are glossed with "形容 or 比喻形容 ...".
    -   Adverbs are glossed with "表..."

-   collocational information, pragmatic information ('tone', etc) are recorded as additional annotation.



![](glossStat.png){fig-align="center"}



## CWN 2.0 Search

-   The most comprehensive and fine-grained sense repository and network in Chinese

. . .

![](lang.png){fig-align="left"}

## CWN 2.0 Programmable Search

-   [API and doc](https://cwngraph.readthedocs.io/en/latest/) freely available


```python
from CwnGraph import CwnImage
cwn = CwnImage.latest()
# the original base data
# from CwnGraph import CwnImage
# cwn = CwnBase()
```

## Zipf's law (no surprise)

-   Most words have small number of senses (Zipf's law)

![](comp.jpg)



## Comparison to other Chinese lexical resources


:::: {.columns .fragment fragment-index=0}


::: {.column width="40%"}

![e-Hownet](ehownet_sen.png)
:::

::: {.column width="40%"}

![Grand Dictionary of Ministry of Education](meng.png)
:::

::: {.column width="40%"}
![CWN20](cwn_sense.png)
:::

::::


## Data summary 1/1


:::: {.columns .fragment fragment-index=1}

::: {.column width="30%"}
![](cwn_sta.png){width=400}
:::

::: {.column width="70%"}


```{r, echo=FALSE, cache=TRUE}
#| label: fig-cwn1
#| fig-cap: cwn sense data summary
#| warning: false
require(tibble)
require(ggplot2)
require(stringr)

df <- tribble(
  ~POS, ~count,
  "Adj", 886,
  "Adv", 1801,
  "N", 12574,
  "V", 13615,
)

ggplot(df) +
  geom_col(aes(x=reorder(POS, -count), y=count, fill=POS)) + 
  labs(x="POS", y="count") + 
  guides(fill="none") +
  geom_text(aes(x=reorder(POS, -count), y=count+400, label=count)) +
  labs(title = str_c("Total Sense Count = ", sum(df$count)))
```
:::

::::


## Data summary 2/2

@fig-cwn2 further demonstrates the distribution of different types of relations

:::: {.columns .fragment fragment-index=1}

::: {.column width="30%"}

![](cwn_sta2.png)
:::

::: {.column width="70%"}

```{r, echo=FALSE, cache=TRUE}
#| label: fig-cwn2
#| fig-cap: cwn relation data summary
#| warning: false

library(tidyverse)
library(ggplot2)

df <- read_csv("./data/cwn_relation_statistics.csv")
df %>% count(relation_type) %>% ggplot() + geom_col(aes(x=reorder(relation_type, n), y=n, fill=relation_type)) + coord_flip() + labs(y="count", x="relation") + guides(fill="none") + theme(axis.title = element_text(size = 16), axis.text = element_text(size = 14)) 
```

:::

::::




## CWN 2.0 
**Visualization**

![](cwnVis.png)


## CWN 2.0
**sense tagger**

- [Transformer-based sense tagger]{#magenta}

-   Leveraging wordnet *glosses* using `GlossBert` [@huang2019glossbert], a BERT model for word sense disambiguation with gloss knowledge.

-   Our extended `GlossBert` model on CWN gloss+ SemCor reports 82% accuracy.

::: notes
conducting *context-gloss* pairs, and fine-tune the pre-trained BERT model on SemCor3.0 training corpus, and achieves SOTA performance on several English all-words WSD tasks.
:::

<img src="glossB.png" alt="drawing" style="width:50px;"/>


## Word Sense Tagger

- APIs (GlossBert version) released in 2021\
<img src="tagger.png" alt="drawing" style="width:400px;"/>

```{python}
#| eval: false
#| echo: true
# pip install -U DistilTag SenseTagger
import DistilTag
import CwnSenseTagger
DistilTag.download()
CwnSenseTagger.download()

tagger = DistilTag()
tagged = tagger.tag("<raw text>")
sense_tagged = senseTag(tagged)
```

![](senseTag.png){fig-align="center"}


## CWN 2.0
**Chinese SemCor**

- semi-automatically curated sense-tagged corpus based on Academic Sinica Balanced Corpus (ASBC) s.

![](senseanno.png){width=80%}




## CWN-based applications
**sense frequency distribution** in corpus

-   Now we have chance to *empirically* explore the **dominance** of word senses in language use, which is essential for both lexical semantic and psycholinguistic studies.

. . .

- e.g., '開' (kai1,'open') has (surprisingly) more dominant *blossom* sense over others (based on randomly chosen 300 sentences in ASBC corpus)

![](senseFreq.png){fig-align="center"}

::: notes
從平衡語料庫中取出 300 句包含「吃」或「開」的句子（一句可能含多個目標詞），用CwnSenseTagger 判斷該詞在脈絡中的詞意，並計算其頻率。以下列出頻率最高的 5 個詞意。
:::


## CWN-based applications
**word sense acquisition**



![credit:郭懷元同學](huan.png)


## CWN-based applications
**tracking sense evolution**

-   The indeterminate nature of Chinese affixoids
-   Sense status of 家 jiā from the Tang dynasty to the 1980s

![](jia_en.png){fig-align="center"}

# [台灣多模態語料庫](https://multimoco.linguistics.ntu.edu.tw/index.html
)
![](moco.png)


# 回到我們要探究的問題

`(Autoregressive) LLMs 與 LR/KR 的關係？`




<!-- - Autoregressive (AR) models, like GPT-3 (and later versions), generate sentences word by word from left to right. They predict the next word in a sentence given all the previous words.  -->

<!-- . . . -->

<!-- - Autoencoding (AE) models, like BERT, instead focus on understanding the sentence as a whole. They are trained to fill in gaps in a sentence. -->




## LLM-based NLP: a new paradigm
Pre-train, Prompt, and Predict [@liu2021pretrain]

![Four paradims in NLP](nlp-paradigm.png)


<!-- ## Foundation Models -->
<!-- From LLMs to FMs (thus [Generative AI]{#magenta}) -->

<!-- ![](foundation_models.png) -->

<!-- A foundation model can centralize the information from all the data from various modalities. This -->
<!-- one model can then be adapted to a wide range of downstream tasks.[@bommasani2021opportunities] -->


<!-- ##  -->

<!-- ::: {.callout-note appearance="default"} -->

<!-- I'd still call it LLMs because of the use of NL -->

<!-- ::: -->



<!-- ## LLM-based NLP  -->
<!-- easier version -->

<!-- :::: {.columns .fragment fragment-index=1} -->

<!-- ::: {.column width="50%"} -->

<!-- ![](nlp-1.png){width="600"} -->


<!-- ![](nlp-2.png){width="600"} -->


<!-- ::: -->

<!-- ::: {.column width="50%"} -->

<!-- ![](nlp-3.png){width="600"} -->
<!-- ::: -->

<!-- :::: -->





## In-context Learning 

> refers to methods for how to communicate with LLM to steer its behavior for desired outcomes without updating the model weights.

<!-- LLM is a general store of information, and in-context learning allows us to access certain parts within this store without having to specialize the entire model towards each task. -->

- Amazing performance with only zero/few-shot
- Requires no parameter updates, just talk to them in [natural language]{#magenta}!
- Prompt engineering: the process of creating a prompt that results in the most effective performance on the downstream task



## Prompt and Prompt engineering

::: {.callout-note appearance="default"}

**prompt**: a natural language description of the task.

**prompt design**: involves instructions and context passed to the LLM to achieve a desired task.

**prompt engineering**: the practice of developing optimal (clear, concise, informative) prompts to efficiently use LLMs for a variety of applications.

:::

::: {.notes}
Prompt Engineering is the way in which instruction and reference data is presented to the LLM.
:::

. . . 

![](prompt_de.png){width=40%}

. . .

> (Text) Generation is a meta capability of Large Language Models & Prompt Engineering is the key to unlocking it. 



  
## Prompt and Prompt engineering
`basic elements`

. . .

A prompt is composed with the following components:


![](prompt_elements.png){width=70%}



[source](www.promptingguide.ai/)

## Prompt and Prompt engineering
`zero and few shot` [@weng2023prompt]
. . .

- Zero shot learning: implies that a model can recognize things that have not explicitly been taught in the training.

- Few shot learning: refers to training a model with minimal data.



## Prompt and Prompt engineering
`Chain-of-Thought` [@wei2023chainofthought] 

- generates a sequence of short sentences to describe reasoning logic step by step, known as reasoning chains or rationales, to eventually lead to the final answer. 

![](cot.png)[source](https://youtu.be/zizonToFXDs)

- zero or few shot CoT



<!-- ## Prompt and Prompt engineering -->
<!-- ``typology`[@weng2023prompt]` -->

<!-- - **Instruction prompting** -->



## Prompt and Prompt engineering
`Self-consistency sampling` [@weng2023prompt]

- to sample multiple outputs with temperature > 0 and then selecting the best one out of these candidates. The criteria for selecting the best candidate can vary from task to task. A general solution is to pick majority vote. 

新發展可以參見 [Prompting Guide](https://www.promptingguide.ai/)



## Prompt and Prompt engineering
`Persona` setting is also important, *socio-linguistically* 

. . .

![awesome-chatgpt-prompts](pe1.png){width=75%}




## Prompting LLM for lexical semantic tasks
`word sense disambiguation`

. . .

![](wsd.jpeg){width="60%"}

## Prompting LLM for solve lexical semantic tasks
`sense to action`

. . .

![](sense2action.jpeg)

## Prompting LLM for solve lexical semantic tasks
`word sense induction`

. . .

![](sense induction.jpeg){width=70%}

## Prompting LLM for solve lexical semantic tasks
`code-switching wsd`

. . .

![](codeswitch.png){width=60%}

## 
 ![](codeswitch2.png){width=60%}




## Prompting LLM for Wordnet data augmentation
**Exploratory Prompting Analysis**

- Prompting LLM becomes an empirical work and the effect of prompt engineering methods can vary a lot among models and tasks, thus requiring heavy experimentation and heuristics.

. . . 

- After exploring different prompt templates, we take a prompt template with bullet points, a sequence of instructions, and a guided inquiring style to be complied with, with the persona setting as "“You are a linguist mastering in lexical semantics and in constructing Chinese Wordnet.”

## 

![](cwn_aug_prompt.png)




## Results
[instruction tuning]{#magenta} 


![](cwn-aug2.png)




## Human ratings

- Human rating is based on the word's appropriateness of interpretation, the meanings' correspondence to the word's part of speech, their avoidance of oversimplification/overgeneralization, and their compliance with the prompt's requirements.

- The top 600 frequent words are rated to further analyze their error types. 


![](rating.png){width=40%}






## Prompting limitations
想辦法問博學者 savant 的各種技能，也會有天花板

> `In-context learning (~ prompting) `involves providing input to the language model in a specific format to elicit the desired output. 

- 提示詞脈絡視窗大小 `context window size` restricts the model's ability to process long sequences of information effectively.

. . .

- 各種幻覺與執著 `Hallucination` appears when the generated content is nonsensical or unfaithful to the provided source content.

  - reluctance to express uncertainty, to change premise, and yields to authority 
  
  - incapable of accessing factual/up-to-date information; or no authentic sources provided
  
## 

- 數據也有可能受到版權、個資與企業隱私問題。


![](llm_freq.png){width=10%}




# A neural-symbolic approach to rebuild the LR ecosystem
[Toward a more linguistic knowledge-aware LLMs]{#magenta}

:::: {.columns .fragment fragment-index=4}

::: {.column width="50%"}

- The neural-symbolic approach seeks to integrate these two paradigms to leverage the strengths of both: the learning and generalization capabilities of neural networks and the interpretability and reasoning capabilities of symbolic systems. 

:::


<!--     - Grounded generation (Indexing LRs): a form of retrieval augmented generation, the LLM provides answers to user queries based not only on the knowledge captured in the dataset the LLM was trained on, but augmented with knowledge coming from searching additional data sources: -->

<!--     - LLM-LR app stores: LR and LLM `langchain`ed could be mutually beneficial for LLM and LR. -->

<!-- - Using cwn and corpus as a case study is a good start. -->



::: {.column width="40%"}


![](augmented-llms.png)
:::
::::




# 目前兩種作法

- **Fine-tuning** on up-to-dated / customized data

. . .

- **Retrieval-augmented Generation** (e.g., RAG prompting)


# Fine-tune

模型的壓縮技術 quantization (`LoRA`, `QLoRA`, ...) 使得微調大型語言模型變得更為可行

## LoLlama: a fine-tuned model

- We fine-tune `LoLlama` on top of Taiwan-LLaMa (Lin and Chen, 2023), which was pre-trained on over
5 billion tokens of Traditional Chinese. The model was further fine-tuned on over 490K multi-turn con-
versational data to enable instruction-following and context-aware responses.
- We train LoLlama with CWN


## Evaluation

![](lollama.png)

## Evaluation
![](lollama2.png)





## 微調的問題與限制

- 商業版本好，但很貴，也不保證安全。

- 開源的 llm 越來越好，壓縮技術越見成熟。但訓練不便宜，結果常動輒被政治價值審查。

. . .

[（抱怨：又要馬兒跑，又要馬兒沒草吃）]{.small-text}


# RAG

> Retrieving facts from an external knowledge base to ground large language models (LLMs) on the most accurate, up-to-date information and to give users insight into LLMs' generative process.

. . .

![](rag.png)


## Vector DataBase and Embeddings

- A (vector) embedding is the internal representation of input data in a deep learning model, also known as embedding models or a deep neural network.

![](vector.png)

- We obtain vectors by removing the last layer and taking the output from the second-to-last layer. 

- Vector DB/ Vector Stores

<!-- ## LLM apps (`Everything app` v.2)  -->

<!-- - `Everything app` (aka super app) is a commercial scenario where all the living services are provided. -->
<!--   - e.g., imaging a super app (like `WeChat`) offers food delivery, ride-hailing, on-demand package delivery, financial and investing, and other services. -->

<!-- . . . -->

<!-- - Similarly, the (external knowledge and factual resources) plugins enable LLMs (like `GPT-4`, `PaLM-2`) to gather real-time information pertinent to their conversation prompts, effectively overcoming the limitation. -->


<!-- ##  LLM apps: another example -->
<!-- `JARVIS` -->

<!-- ![HuggingGPT: workflow](hugginggpt_workflow.png) -->

<!-- ##  -->

<!-- - an LLM as the controller and numerous expert models as collaborative executors (from HuggingFace Hub), demo at https://huggingface.co/spaces/microsoft/HuggingGPT -->

<!-- ![](hf_jarvis.jpeg) -->



## Workflow

![](langchain-step.png)

# [lopeGPT]{#magenta}: [a RAG model](https://lope.linguistics.ntu.edu.tw/lopeGPT)

:::: {.columns} 

::: {.column width="50%"} 

<!-- <img src="lopegpt.png" alt="drawing" align = "center" style="width:900px;"/> -->
a higer archtecture
![](lopegpt.png){width="920px"}

:::



::: {.column width="50%"} 
- Integration of language resources: 
  - Academia Sinica Balanced Corpus of Modern Chinese (ASBC)
  - Social Media Corpus in Taiwan (SoMe)
  - Chinese Wordnet 2.0 (CWN)


![](lopegpt-rag.png)
<br>
[https://lope.linguistics.ntu.edu.tw/lopeGPT]()

:::
::::


## Experiments on Sense Computing Tasks
`augmented LLMs`

- sense definition, lexical relation query and processing
- sense tagging and analysis 

## 
![](lopegpt-3.png)


##
![](lopegpt-2.png)




## Experiments on Sense computing tasks
`localized and customized`

- upload data to calculate (word frequency, word sense frequency, etc) via `llama-index` data loader.

- vectorized the data and semantically search/compare 

- given few shot, predict the sense and automatically generate the gloss (and relations to others)

. . .

::: {.callout-note appearance="default"}

All examples are tested with `chatgpt-3.5-turbo` (using OpenAI's API) . It uses the default configurations, i.e., `temperature`=0.0.

:::


## Some prelimenary results

![](lopegpt_upload.png)




# LLMs orchestration
Orchestration frameworks provide a way to manage and control LLMs. 

需要一名專案經理，協調所有進行中的工作項目，確保各專案達成理想的最終結果


## LLMs orchestration frameworks

- LlamaIndex 

- Semantic Kernel

- LangChain

## Comparison

![](sk.png)

## Langchain

> `langchain` is an open source framework that allows AI developers to combine LLMs like GPT-4 with external sources of computation and data.

![Github repo star history](star.png){width="50%"}



## Architecture

![](lanchain.gif)

## langchain components

- **Chains**: The core of `langchain`. Components (and even other chains) can be stringed together to create *chains*.

- **Prompt templates**: Prompt templates are templates for different types of prompts. Like “chatbot” style templates, ELI5 question-answering, etc

- **LLMs**: Large language models

- **Indexing Utils:** Ways to interact with specific data (embeddings, vectorstores, document loaders)

- **Tools:** Ways to interact with the outside world (search, calculators, etc)

- [**Agents**: Agents use LLMs to decide what actions should be taken]{#magenta}. Tools like web search or calculators can be used, and all are packaged into a logical loop of operations.

- **Memory**: Short-term memory, long-term memory.




## Working with LLMs and your own data

-   Good news for Language and Knowledge Resource developers

<!-- Be data-aware: connect a language model to other sources of data -->
<!-- Be agentic: allow a language model to interact with its environment -->

. . .

![](langchain.png){width=60%}


<!-- ## Chain -->

<!-- 有點像組合函數，裏面是 prompt template, 外面是 llm -->



<!-- ## Next:  -->

<!-- 如此一來，Auto-GPT 就能打造出一個閉環（closed loop），進而有能力應對更複雜的多步程序；我們也可以把 Auto-GPT 看作一名專案經理，協調所有進行中的工作項目，確保各專案達成理想的最終結果。目前 Auto-GPT 能夠與線上和本地的應用程序、軟體和服務進行各種互動，例如網頁瀏覽器與文字處理器。 . . . -->

<!-- Once you have explored and want to bring it into memory, it's also fast! -->


<!-- # Final thoughts -->

<!-- . . . -->

<!-- - Democratizing LLM and connect LR help keep balance between the generalization capabilities of neural networks and the interpretability and reasoning capabilities of symbolic knowledge systems.  -->
<!--     - LLM chains; Agent network -->

<!-- . . . -->

<!-- - [Auto-GPT]{#magenta} and alike as Project Manager (for a scaled and manageable ecosystem for `LR + LLM` based applications) -->

<!-- . . . -->

<!-- > More and more democratized LLMs and more diverse ecosystems which lead to a more stable symbiosis for Human and Machine. -->




<!-- # Acknowledgment -->


<!-- > 連大成、陳品而、王伯雅、古貿昌、張淳涵以及 lopers 們！ -->

## 總結

- 語言與知識資源與模型訓練一樣重要：可信任、可解釋、可克制（與客製）。

. . .

- 兩者連結的可能目前是 Fine-tune 與 RAG prompting。

. . .

- 了解 orchestration 的架構 (e.g.，`langchain`) 對於部署 LLM 應用變成核心技能。



## Reference

::: {#refs}
:::



<!-- # [祝黃老師 福如東海 壽比南山]{#magenta} -->

<!-- <iframe data-external="1" src="end.mp4" width="100%" height="85%"></iframe> -->

<!-- {{< "video" "end.mp4" "width"="100%" "height"="85%" >}} -->


## Hands-on coding tutorial
:::: {.columns} 

::: {.column width="50%"} 
![](langchain-tutorial.png){width=35%}

<br>
[colab notebook 連結](https://t.ly/p3Z4_)

:::


::: {.column width="50%"} 
![](piner.jpg){width=200}

![](richard.jpg)
:::

::::
